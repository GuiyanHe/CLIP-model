{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-18T17:00:36.190455359Z",
     "start_time": "2026-02-18T16:59:47.370905019Z"
    }
   },
   "source": [
    "import onnx\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# ç¦ç”¨æ—¥å¿—\n",
    "os.environ['ORT_LOG_LEVEL'] = '3'\n",
    "logging.getLogger(\"onnxruntime\").setLevel(logging.ERROR)\n",
    "\n",
    "MODEL_DIR = Path(\"../../model\")\n",
    "FP32_PATH = MODEL_DIR / \"clip-image-encoder.onnx\"\n",
    "COCO_VAL_DIR = Path(\"../../data/dataset/coco/val2017\")\n",
    "\n",
    "# ==================== GPU é…ç½® ====================\n",
    "print(\"ğŸ”§ é…ç½® GPU æ¨ç†...\")\n",
    "\n",
    "providers = [\n",
    "    ('CUDAExecutionProvider', {'device_id': 0}),\n",
    "    'CPUExecutionProvider'\n",
    "]\n",
    "\n",
    "# ==================== åŠ è½½å›¾åƒæ•°æ® ====================\n",
    "print(\"\\nğŸ“¦ åŠ è½½ COCO å›¾åƒæ•°æ®...\")\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073),\n",
    "                        (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "image_paths = sorted(list(COCO_VAL_DIR.glob(\"*.jpg\")))[:500]\n",
    "print(f\"âœ… åŠ è½½ {len(image_paths)} å¼ å›¾åƒ\")\n",
    "\n",
    "image_tensors = []\n",
    "for img_p in tqdm(image_paths, desc=\"Loading images\"):\n",
    "    img = Image.open(img_p).convert(\"RGB\")\n",
    "    tensor = preprocess(img).unsqueeze(0).numpy()\n",
    "    image_tensors.append(tensor)\n",
    "\n",
    "print(f\"âœ… å›¾åƒåŠ è½½å®Œæˆ\")\n",
    "\n",
    "# ==================== èŠ‚ç‚¹åˆ†æ ====================\n",
    "print(\"\\nğŸ” åˆ†æ ONNX èŠ‚ç‚¹ç»“æ„...\")\n",
    "\n",
    "model = onnx.load(str(FP32_PATH))\n",
    "\n",
    "node_types = {}\n",
    "for node in model.graph.node:\n",
    "    node_types.setdefault(node.op_type, []).append(node.name)\n",
    "\n",
    "print(f\"\\nğŸ“Š èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡:\")\n",
    "for op_type, nodes in sorted(node_types.items(), key=lambda x: -len(x[1])):\n",
    "    print(f\"   {op_type:15s}: {len(nodes):3d} ä¸ª\")\n",
    "\n",
    "conv_nodes = node_types.get('Conv', [])\n",
    "matmul_nodes = node_types.get('MatMul', [])\n",
    "gemm_nodes = node_types.get('Gemm', [])\n",
    "softmax_nodes = node_types.get('Softmax', [])\n",
    "\n",
    "print(f\"\\nğŸ¯ å…³é”®èŠ‚ç‚¹:\")\n",
    "print(f\"   Conv: {len(conv_nodes)}, MatMul: {len(matmul_nodes)}, Gemm: {len(gemm_nodes)}, Softmax: {len(softmax_nodes)}\")\n",
    "\n",
    "# ==================== é…ç½® ONNX Runtime ====================\n",
    "opts = ort.SessionOptions()\n",
    "opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "opts.intra_op_num_threads = 8\n",
    "\n",
    "# ==================== è·å– FP32 åŸºå‡† ====================\n",
    "print(f\"\\nğŸ“Š è·å– FP32 åŸºå‡†ç‰¹å¾ï¼ˆGPUï¼‰...\")\n",
    "\n",
    "sess_fp32 = ort.InferenceSession(str(FP32_PATH), opts, providers=providers)\n",
    "input_name = sess_fp32.get_inputs()[0].name\n",
    "\n",
    "print(f\"   æä¾›è€…: {sess_fp32.get_providers()}\")\n",
    "\n",
    "for _ in range(3):\n",
    "    _ = sess_fp32.run(None, {input_name: image_tensors[0]})\n",
    "\n",
    "print(\"   FP32 æ¨ç†...\")\n",
    "fp32_feats = []\n",
    "for img_tensor in tqdm(image_tensors, desc=\"FP32 inference\"):\n",
    "    output = sess_fp32.run(None, {input_name: img_tensor})[0]\n",
    "    fp32_feats.append(output)\n",
    "\n",
    "fp32_feats = np.concatenate(fp32_feats)\n",
    "fp32_norm = fp32_feats / (np.linalg.norm(fp32_feats, axis=-1, keepdims=True) + 1e-7)\n",
    "\n",
    "print(f\"âœ… FP32 ç‰¹å¾: {fp32_feats.shape}\")\n",
    "\n",
    "# ==================== å®šä¹‰æµ‹è¯•å‡½æ•° ====================\n",
    "\n",
    "def get_cos_sim(model_path):\n",
    "    \"\"\"è®¡ç®—ä¸ FP32 çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆGPUï¼‰\"\"\"\n",
    "    if not model_path.exists():\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        sess = ort.InferenceSession(str(model_path), opts, providers=providers)\n",
    "        input_name = sess.get_inputs()[0].name\n",
    "\n",
    "        for _ in range(2):\n",
    "            _ = sess.run(None, {input_name: image_tensors[0]})\n",
    "\n",
    "        int8_feats = []\n",
    "        for img_tensor in image_tensors:\n",
    "            output = sess.run(None, {input_name: img_tensor})[0]\n",
    "            int8_feats.append(output)\n",
    "\n",
    "        int8_feats = np.concatenate(int8_feats)\n",
    "        int8_norm = int8_feats / (np.linalg.norm(int8_feats, axis=-1, keepdims=True) + 1e-7)\n",
    "\n",
    "        return np.mean(np.sum(fp32_norm * int8_norm, axis=-1))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ==================== ç­–ç•¥ A: é€ä¸ªæ’é™¤ Conv ====================\n",
    "print(\"\\n\\nğŸ§ª ç­–ç•¥ A: é€ä¸ªæ’é™¤ Conv èŠ‚ç‚¹\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "temp_dir = MODEL_DIR / \"temp_tests\"\n",
    "temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# å…ˆæµ‹è¯•å…¨é‡åŒ–\n",
    "test_path = temp_dir / \"all_quantized.onnx\"\n",
    "quantize_dynamic(\n",
    "    str(FP32_PATH),\n",
    "    str(test_path),\n",
    "    weight_type=QuantType.QInt8,\n",
    "    nodes_to_exclude=[]\n",
    ")\n",
    "\n",
    "sim_baseline = get_cos_sim(test_path)\n",
    "\n",
    "if sim_baseline is None:\n",
    "    print(\"âŒ å…¨é‡åŒ–å¤±è´¥ï¼Œæ’é™¤æ‰€æœ‰ Conv é‡è¯•\")\n",
    "    test_path.unlink()\n",
    "    test_path = temp_dir / \"all_quantized_no_conv.onnx\"\n",
    "    quantize_dynamic(\n",
    "        str(FP32_PATH),\n",
    "        str(test_path),\n",
    "        weight_type=QuantType.QInt8,\n",
    "        nodes_to_exclude=conv_nodes\n",
    "    )\n",
    "    sim_baseline = get_cos_sim(test_path)\n",
    "    base_exclude = conv_nodes.copy()\n",
    "    print(f\"ğŸ”´ æ’é™¤æ‰€æœ‰ Conv: {sim_baseline:.4f}\\n\")\n",
    "else:\n",
    "    base_exclude = []\n",
    "    print(f\"ğŸ”´ å…¨é‡åŒ–åŸºå‡†: {sim_baseline:.4f}\\n\")\n",
    "\n",
    "test_path.unlink()\n",
    "\n",
    "results_conv = []\n",
    "\n",
    "print(\"é€ä¸ªæ’é™¤ Conv èŠ‚ç‚¹:\\n\")\n",
    "for i, conv_name in enumerate(conv_nodes):\n",
    "    # âœ… ç´¯ç§¯æ’é™¤ï¼šæ’é™¤å‰ i+1 ä¸ª Conv\n",
    "    exclude_list = base_exclude + conv_nodes[:i+1]\n",
    "    test_path = temp_dir / f\"exclude_conv_{i}.onnx\"\n",
    "\n",
    "    quantize_dynamic(\n",
    "        str(FP32_PATH),\n",
    "        str(test_path),\n",
    "        weight_type=QuantType.QInt8,\n",
    "        nodes_to_exclude=exclude_list\n",
    "    )\n",
    "\n",
    "    sim = get_cos_sim(test_path)\n",
    "    results_conv.append((f\"æ’é™¤å‰ {i+1} ä¸ª Conv\", sim))\n",
    "\n",
    "    if sim is not None:\n",
    "        delta = sim - sim_baseline\n",
    "        marker = \"ğŸ“ˆ\" if delta > 0.01 else \"âœ“\" if delta > -0.005 else \"ğŸ“‰\"\n",
    "        print(f\"   æ’é™¤å‰ {i+1:2d} ä¸ª Conv (å« {conv_name}): {sim:.4f} ({delta:+.4f}) {marker}\")\n",
    "\n",
    "    test_path.unlink()\n",
    "\n",
    "# ==================== ç­–ç•¥ B: é€ä¸ªæ’é™¤ MatMul ====================\n",
    "print(\"\\n\\nğŸ§ª ç­–ç•¥ B: é€ä¸ªæ’é™¤ MatMul èŠ‚ç‚¹\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_matmul = []\n",
    "\n",
    "print(\"\\né€ä¸ªæ’é™¤ MatMul èŠ‚ç‚¹:\\n\")\n",
    "for i, matmul_name in enumerate(matmul_nodes):\n",
    "    # âœ… ç´¯ç§¯æ’é™¤ï¼šæ’é™¤å‰ i+1 ä¸ª MatMul\n",
    "    exclude_list = base_exclude + matmul_nodes[:i+1]\n",
    "    test_path = temp_dir / f\"exclude_matmul_{i}.onnx\"\n",
    "\n",
    "    quantize_dynamic(\n",
    "        str(FP32_PATH),\n",
    "        str(test_path),\n",
    "        weight_type=QuantType.QInt8,\n",
    "        nodes_to_exclude=exclude_list\n",
    "    )\n",
    "\n",
    "    sim = get_cos_sim(test_path)\n",
    "    results_matmul.append((f\"æ’é™¤å‰ {i+1} ä¸ª MatMul\", sim))\n",
    "\n",
    "    if sim is not None:\n",
    "        delta = sim - sim_baseline\n",
    "        marker = \"ğŸ“ˆ\" if delta > 0.01 else \"âœ“\" if delta > -0.005 else \"ğŸ“‰\"\n",
    "        print(f\"   æ’é™¤å‰ {i+1:2d} ä¸ª MatMul (å« {matmul_name}): {sim:.4f} ({delta:+.4f}) {marker}\")\n",
    "\n",
    "    test_path.unlink()\n",
    "\n",
    "# ==================== ç­–ç•¥ C: é€ä¸ªæ’é™¤ Gemm ====================\n",
    "print(\"\\n\\nğŸ§ª ç­–ç•¥ C: é€ä¸ªæ’é™¤ Gemm èŠ‚ç‚¹\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_gemm = []\n",
    "\n",
    "print(\"\\né€ä¸ªæ’é™¤ Gemm èŠ‚ç‚¹:\\n\")\n",
    "for i, gemm_name in enumerate(gemm_nodes):\n",
    "    # âœ… ç´¯ç§¯æ’é™¤ï¼šæ’é™¤å‰ i+1 ä¸ª Gemm\n",
    "    exclude_list = base_exclude + gemm_nodes[:i+1]\n",
    "    test_path = temp_dir / f\"exclude_gemm_{i}.onnx\"\n",
    "\n",
    "    quantize_dynamic(\n",
    "        str(FP32_PATH),\n",
    "        str(test_path),\n",
    "        weight_type=QuantType.QInt8,\n",
    "        nodes_to_exclude=exclude_list\n",
    "    )\n",
    "\n",
    "    sim = get_cos_sim(test_path)\n",
    "    results_gemm.append((f\"æ’é™¤å‰ {i+1} ä¸ª Gemm\", sim))\n",
    "\n",
    "    if sim is not None:\n",
    "        delta = sim - sim_baseline\n",
    "        marker = \"ğŸ“ˆ\" if delta > 0.01 else \"âœ“\" if delta > -0.005 else \"ğŸ“‰\"\n",
    "        print(f\"   æ’é™¤å‰ {i+1:2d} ä¸ª Gemm (å« {gemm_name}): {sim:.4f} ({delta:+.4f}) {marker}\")\n",
    "\n",
    "    test_path.unlink()\n",
    "\n",
    "# ==================== ç­–ç•¥ D: é€ä¸ªæ’é™¤ Softmax ====================\n",
    "print(\"\\n\\nğŸ§ª ç­–ç•¥ D: é€ä¸ªæ’é™¤ Softmax èŠ‚ç‚¹\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_softmax = []\n",
    "\n",
    "print(\"\\né€ä¸ªæ’é™¤ Softmax èŠ‚ç‚¹:\\n\")\n",
    "for i, softmax_name in enumerate(softmax_nodes):\n",
    "    # âœ… ç´¯ç§¯æ’é™¤ï¼šæ’é™¤å‰ i+1 ä¸ª Softmax\n",
    "    exclude_list = base_exclude + softmax_nodes[:i+1]\n",
    "    test_path = temp_dir / f\"exclude_softmax_{i}.onnx\"\n",
    "\n",
    "    quantize_dynamic(\n",
    "        str(FP32_PATH),\n",
    "        str(test_path),\n",
    "        weight_type=QuantType.QInt8,\n",
    "        nodes_to_exclude=exclude_list\n",
    "    )\n",
    "\n",
    "    sim = get_cos_sim(test_path)\n",
    "    results_softmax.append((f\"æ’é™¤å‰ {i+1} ä¸ª Softmax\", sim))\n",
    "\n",
    "    if sim is not None:\n",
    "        delta = sim - sim_baseline\n",
    "        marker = \"ğŸ“ˆ\" if delta > 0.01 else \"âœ“\" if delta > -0.005 else \"ğŸ“‰\"\n",
    "        print(f\"   æ’é™¤å‰ {i+1:2d} ä¸ª Softmax (å« {softmax_name}): {sim:.4f} ({delta:+.4f}) {marker}\")\n",
    "\n",
    "    test_path.unlink()\n",
    "\n",
    "# ==================== æ€»ç»“ ====================\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š å®Œæ•´è¯Šæ–­æ€»ç»“\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nåŸºå‡†ç›¸ä¼¼åº¦: {sim_baseline:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ¥‡ å„ç­–ç•¥æœ€ä¼˜ç»“æœ:\")\n",
    "\n",
    "if results_conv:\n",
    "    best_conv = max(results_conv, key=lambda x: x[1])\n",
    "    print(f\"\\n  Conv ç­–ç•¥: {best_conv[0]}\")\n",
    "    print(f\"  ç›¸ä¼¼åº¦: {best_conv[1]:.4f}\")\n",
    "\n",
    "if results_matmul:\n",
    "    best_matmul = max(results_matmul, key=lambda x: x[1])\n",
    "    print(f\"\\n  MatMul ç­–ç•¥: {best_matmul[0]}\")\n",
    "    print(f\"  ç›¸ä¼¼åº¦: {best_matmul[1]:.4f}\")\n",
    "\n",
    "if results_gemm:\n",
    "    best_gemm = max(results_gemm, key=lambda x: x[1])\n",
    "    print(f\"\\n  Gemm ç­–ç•¥: {best_gemm[0]}\")\n",
    "    print(f\"  ç›¸ä¼¼åº¦: {best_gemm[1]:.4f}\")\n",
    "\n",
    "if results_softmax:\n",
    "    best_softmax = max(results_softmax, key=lambda x: x[1])\n",
    "    print(f\"\\n  Softmax ç­–ç•¥: {best_softmax[0]}\")\n",
    "    print(f\"  ç›¸ä¼¼åº¦: {best_softmax[1]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# æ¸…ç†\n",
    "shutil.rmtree(temp_dir)\n",
    "\n",
    "print(\"\\nâœ… è¯Šæ–­å®Œæˆï¼\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ é…ç½® GPU æ¨ç†...\n",
      "\n",
      "ğŸ“¦ åŠ è½½ COCO å›¾åƒæ•°æ®...\n",
      "âœ… åŠ è½½ 500 å¼ å›¾åƒ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 253.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å›¾åƒåŠ è½½å®Œæˆ\n",
      "\n",
      "ğŸ” åˆ†æ ONNX èŠ‚ç‚¹ç»“æ„...\n",
      "\n",
      "ğŸ“Š èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡:\n",
      "   Reshape        : 109 ä¸ª\n",
      "   Transpose      :  75 ä¸ª\n",
      "   Add            :  61 ä¸ª\n",
      "   MatMul         :  61 ä¸ª\n",
      "   Mul            :  48 ä¸ª\n",
      "   Gather         :  37 ä¸ª\n",
      "   LayerNormalization:  26 ä¸ª\n",
      "   Unsqueeze      :  12 ä¸ª\n",
      "   Squeeze        :  12 ä¸ª\n",
      "   Softmax        :  12 ä¸ª\n",
      "   Gemm           :  12 ä¸ª\n",
      "   Sigmoid        :  12 ä¸ª\n",
      "   Conv           :   1 ä¸ª\n",
      "   Concat         :   1 ä¸ª\n",
      "\n",
      "ğŸ¯ å…³é”®èŠ‚ç‚¹:\n",
      "   Conv: 1, MatMul: 61, Gemm: 12, Softmax: 12\n",
      "\n",
      "ğŸ“Š è·å– FP32 åŸºå‡†ç‰¹å¾ï¼ˆGPUï¼‰...\n",
      "   æä¾›è€…: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "   FP32 æ¨ç†...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FP32 inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:00<00:00, 541.72it/s]\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FP32 ç‰¹å¾: (500, 512)\n",
      "\n",
      "\n",
      "ğŸ§ª ç­–ç•¥ A: é€ä¸ªæ’é™¤ Conv èŠ‚ç‚¹\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1;31m2026-02-18 10:59:53.113704206 [E:onnxruntime:, inference_session.cc:2544 operator()] Exception during initialization: /onnxruntime_src/onnxruntime/core/optimizer/transformer_memcpy.cc:254 bool onnxruntime::TransformerMemcpyImpl::IsNodeCompatibleWithProvider(const onnxruntime::Node&) const node_provider != nullptr was false. Unable to get provider associated with provider type \n",
      "\u001B[m\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ å…¨é‡åŒ–å¤±è´¥ï¼Œæ’é™¤æ‰€æœ‰ Conv é‡è¯•\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2026-02-18 10:59:53.996221614 [W:onnxruntime:, transformer_memcpy.cc:111 ApplyImpl] 147 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001B[m\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”´ æ’é™¤æ‰€æœ‰ Conv: 0.9896\n",
      "\n",
      "é€ä¸ªæ’é™¤ Conv èŠ‚ç‚¹:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2026-02-18 11:00:02.081539443 [W:onnxruntime:, transformer_memcpy.cc:111 ApplyImpl] 147 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001B[m\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   æ’é™¤å‰  1 ä¸ª Conv (å« node_Conv_1095): 0.9896 (+0.0000) âœ“\n",
      "\n",
      "\n",
      "ğŸ§ª ç­–ç•¥ B: é€ä¸ªæ’é™¤ MatMul èŠ‚ç‚¹\n",
      "======================================================================\n",
      "\n",
      "é€ä¸ªæ’é™¤ MatMul èŠ‚ç‚¹:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2026-02-18 11:00:07.901389811 [W:onnxruntime:, transformer_memcpy.cc:111 ApplyImpl] 144 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001B[m\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   æ’é™¤å‰  1 ä¸ª MatMul (å« node_MatMul_18): 0.9898 (+0.0001) âœ“\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2026-02-18 11:00:14.588294517 [W:onnxruntime:, transformer_memcpy.cc:111 ApplyImpl] 144 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001B[m\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   æ’é™¤å‰  2 ä¸ª MatMul (å« node_MatMul_90): 0.9898 (+0.0001) âœ“\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2026-02-18 11:00:22.735202604 [W:onnxruntime:, transformer_memcpy.cc:111 ApplyImpl] 144 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001B[m\n",
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   æ’é™¤å‰  3 ä¸ª MatMul (å« node_scaled_dot_product_attention): 0.9898 (+0.0001) âœ“\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2026-02-18 11:00:29.994603750 [W:onnxruntime:, transformer_memcpy.cc:111 ApplyImpl] 141 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001B[m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   æ’é™¤å‰  4 ä¸ª MatMul (å« node_MatMul_102): 0.9904 (+0.0008) âœ“\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../model/temp_tests/exclude_matmul_3.onnx'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 215\u001B[0m\n\u001B[1;32m    212\u001B[0m         marker \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mğŸ“ˆ\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m delta \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.01\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mâœ“\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m delta \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.005\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mğŸ“‰\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    213\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m   æ’é™¤å‰ \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m2d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ä¸ª MatMul (å« \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmatmul_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msim\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdelta\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m+.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmarker\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 215\u001B[0m     \u001B[43mtest_path\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munlink\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;66;03m# ==================== ç­–ç•¥ C: é€ä¸ªæ’é™¤ Gemm ====================\u001B[39;00m\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mğŸ§ª ç­–ç•¥ C: é€ä¸ªæ’é™¤ Gemm èŠ‚ç‚¹\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/clip/lib/python3.10/pathlib.py:1206\u001B[0m, in \u001B[0;36mPath.unlink\u001B[0;34m(self, missing_ok)\u001B[0m\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1202\u001B[0m \u001B[38;5;124;03mRemove this file or link.\u001B[39;00m\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;124;03mIf the path is a directory, use rmdir() instead.\u001B[39;00m\n\u001B[1;32m   1204\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1205\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1206\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_accessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munlink\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1207\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n\u001B[1;32m   1208\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m missing_ok:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../../model/temp_tests/exclude_matmul_3.onnx'"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
