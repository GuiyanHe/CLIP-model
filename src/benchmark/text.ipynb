{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-18T17:03:31.702906336Z",
     "start_time": "2026-02-18T17:00:13.325615594Z"
    }
   },
   "source": [
    "import onnx\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType, quant_pre_process\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import clip\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "MODEL_DIR = Path(\"../../model\")\n",
    "FP32_PATH = MODEL_DIR / \"clip-text-encoder.onnx\"\n",
    "COCO_ANN_JSON = Path(\"../../data/dataset/coco/annotations/captions_val2017.json\")\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "with open(COCO_ANN_JSON, 'r') as f:\n",
    "    queries = [ann['caption'] for ann in json.load(f)['annotations'][:100]]\n",
    "tokenized_data = [clip.tokenize([q]).numpy().astype(np.int64) for q in queries]\n",
    "\n",
    "# ==================== ç¬¬ä¸€æ­¥ï¼šå®Œæ•´èŠ‚ç‚¹æ ‘åˆ†æ ====================\n",
    "print(\"ğŸ” åˆ†æ ONNX èŠ‚ç‚¹ç»“æ„...\")\n",
    "model = onnx.load(str(FP32_PATH.parent / \"clip-text-encoder-quant-pre.onnx\"))\n",
    "\n",
    "# æ‰“å°æ‰€æœ‰èŠ‚ç‚¹ï¼ŒæŒ‰ç±»å‹åˆ†ç±»\n",
    "node_types = {}\n",
    "node_names_all = []\n",
    "\n",
    "for node in model.graph.node:\n",
    "    node_types.setdefault(node.op_type, []).append(node.name)\n",
    "    node_names_all.append(node.name)\n",
    "\n",
    "print(f\"\\nğŸ“Š èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡:\")\n",
    "for op_type, nodes in sorted(node_types.items(), key=lambda x: -len(x[1])):\n",
    "    print(f\"   {op_type:15s}: {len(nodes):3d} ä¸ª\")\n",
    "    if len(nodes) <= 3:\n",
    "        for n in nodes:\n",
    "            print(f\"      â””â”€ {n}\")\n",
    "\n",
    "# å…³é”®ï¼šæ‰¾åˆ°æ‰€æœ‰ MatMul èŠ‚ç‚¹\n",
    "matmul_nodes = node_types.get('MatMul', [])\n",
    "print(f\"\\nğŸ¯ æ‰€æœ‰ MatMul èŠ‚ç‚¹ ({len(matmul_nodes)} ä¸ª):\")\n",
    "for i, node_name in enumerate(matmul_nodes):\n",
    "    print(f\"   {i}: {node_name}\")\n",
    "\n",
    "# ==================== ç¬¬äºŒæ­¥ï¼šæŒ‰èŠ‚ç‚¹é¡ºåºé€ä¸ªé‡åŒ–æµ‹è¯• ====================\n",
    "print(\"\\n\\nğŸ§ª æŒ‰èŠ‚ç‚¹æ’åºï¼Œé€ä¸ªé‡åŒ–æµ‹è¯•...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def get_similarity(model_path):\n",
    "    \"\"\"è®¡ç®—ä¸ FP32 çš„ç›¸ä¼¼åº¦\"\"\"\n",
    "    if not model_path.exists():\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        sess = ort.InferenceSession(str(model_path),\n",
    "                                    providers=['CPUExecutionProvider'])\n",
    "        input_name = sess.get_inputs()[0].name\n",
    "\n",
    "        similarities = []\n",
    "        for token_batch in tokenized_data[:20]:  # åªç”¨20æ¡æµ‹è¯•ï¼ŒåŠ å¿«é€Ÿåº¦\n",
    "            output = sess.run(None, {input_name: token_batch})[0]\n",
    "            output_norm = output / (np.linalg.norm(output, axis=-1, keepdims=True) + 1e-7)\n",
    "\n",
    "            similarities.append(output_norm)\n",
    "\n",
    "        return np.mean(np.concatenate(similarities))\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ é”™è¯¯: {e}\")\n",
    "        return None\n",
    "\n",
    "# è·å– FP32 åŸºå‡†\n",
    "sess_fp32 = ort.InferenceSession(str(FP32_PATH),\n",
    "                                 providers=['CPUExecutionProvider'])\n",
    "input_name = sess_fp32.get_inputs()[0].name\n",
    "fp32_feats = []\n",
    "for token_batch in tokenized_data[:20]:\n",
    "    output = sess_fp32.run(None, {input_name: token_batch})[0]\n",
    "    output_norm = output / (np.linalg.norm(output, axis=-1, keepdims=True) + 1e-7)\n",
    "    fp32_feats.append(output_norm)\n",
    "fp32_feats = np.concatenate(fp32_feats)\n",
    "\n",
    "def get_cos_sim(model_path):\n",
    "    \"\"\"æ›´ç²¾å‡†çš„ç›¸ä¼¼åº¦è®¡ç®—\"\"\"\n",
    "    if not model_path.exists():\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        sess = ort.InferenceSession(str(model_path),\n",
    "                                    providers=['CPUExecutionProvider'])\n",
    "        input_name = sess.get_inputs()[0].name\n",
    "\n",
    "        int8_feats = []\n",
    "        for token_batch in tokenized_data[:20]:\n",
    "            output = sess.run(None, {input_name: token_batch})[0]\n",
    "            output_norm = output / (np.linalg.norm(output, axis=-1, keepdims=True) + 1e-7)\n",
    "            int8_feats.append(output_norm)\n",
    "        int8_feats = np.concatenate(int8_feats)\n",
    "\n",
    "        return np.mean(np.sum(fp32_feats * int8_feats, axis=-1))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ==================== å…³é”®ç­–ç•¥ï¼šé€ä¸ª MatMul æ·»åŠ æ’é™¤ ====================\n",
    "print(\"\\nç­–ç•¥ A: é€ä¸ªæ’é™¤ MatMul èŠ‚ç‚¹\\n\")\n",
    "\n",
    "results_matmul = []\n",
    "temp_dir = MODEL_DIR / \"temp_tests\"\n",
    "temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# å…ˆæµ‹è¯•å…¨é‡åŒ–\n",
    "test_path = temp_dir / \"all_quantized.onnx\"\n",
    "quantize_dynamic(\n",
    "    str(FP32_PATH.parent / \"clip-text-encoder-quant-pre.onnx\"),\n",
    "    str(test_path),\n",
    "    weight_type=QuantType.QInt8,\n",
    "    nodes_to_exclude=['node_GatherND_1006']  # åªæ’é™¤ GatherND\n",
    ")\n",
    "sim_baseline = get_cos_sim(test_path)\n",
    "print(f\"ğŸ”´ å…¨é‡åŒ– (ä»…æ’é™¤ GatherND): {sim_baseline:.4f}\")\n",
    "results_matmul.append(('å…¨é‡åŒ–', sim_baseline))\n",
    "test_path.unlink()\n",
    "\n",
    "# é€ä¸ªæ’é™¤ MatMul\n",
    "for i, matmul_name in enumerate(matmul_nodes):\n",
    "    exclude_list = ['node_GatherND_1006'] + matmul_nodes[:i+1]\n",
    "\n",
    "    test_path = temp_dir / f\"exclude_matmul_{i}.onnx\"\n",
    "    quantize_dynamic(\n",
    "        str(FP32_PATH.parent / \"clip-text-encoder-quant-pre.onnx\"),\n",
    "        str(test_path),\n",
    "        weight_type=QuantType.QInt8,\n",
    "        nodes_to_exclude=exclude_list\n",
    "    )\n",
    "\n",
    "    sim = get_cos_sim(test_path)\n",
    "    results_matmul.append((f\"æ’é™¤å‰ {i+1} ä¸ª MatMul\", sim))\n",
    "\n",
    "    print(f\"   æ’é™¤å‰ {i+1:2d} ä¸ª MatMul (å« {matmul_name}): {sim:.4f}\")\n",
    "\n",
    "    test_path.unlink()\n",
    "\n",
    "# ==================== ç­–ç•¥ B: é€ä¸ª Softmax æ’é™¤ ====================\n",
    "print(\"\\nç­–ç•¥ B: é€ä¸ªæ’é™¤ Softmax èŠ‚ç‚¹\\n\")\n",
    "\n",
    "softmax_nodes = node_types.get('Softmax', [])\n",
    "print(f\"Softmax èŠ‚ç‚¹æ•°: {len(softmax_nodes)}\\n\")\n",
    "\n",
    "results_softmax = []\n",
    "\n",
    "# å…ˆæ’é™¤æ‰€æœ‰ Softmax\n",
    "exclude_list = ['node_GatherND_1006'] + softmax_nodes\n",
    "test_path = temp_dir / \"exclude_all_softmax.onnx\"\n",
    "quantize_dynamic(\n",
    "    str(FP32_PATH.parent / \"clip-text-encoder-quant-pre.onnx\"),\n",
    "    str(test_path),\n",
    "    weight_type=QuantType.QInt8,\n",
    "    nodes_to_exclude=exclude_list\n",
    ")\n",
    "sim_all_softmax = get_cos_sim(test_path)\n",
    "print(f\"ğŸŸ¡ æ’é™¤æ‰€æœ‰ Softmax: {sim_all_softmax:.4f}\")\n",
    "results_softmax.append(('æ’é™¤æ‰€æœ‰ Softmax', sim_all_softmax))\n",
    "test_path.unlink()\n",
    "\n",
    "# ==================== ç­–ç•¥ C: è´ªå¿ƒæœç´¢ ====================\n",
    "print(\"\\nç­–ç•¥ C: è´ªå¿ƒæœç´¢ï¼ˆæ‰¾æœ€æ•æ„Ÿçš„èŠ‚ç‚¹ï¼‰\\n\")\n",
    "\n",
    "base_exclude = ['node_GatherND_1006', 'node_matmul']  # åŸºç¡€æ’é™¤\n",
    "current_exclude = base_exclude.copy()\n",
    "current_sim = sim_baseline\n",
    "\n",
    "remaining_nodes = [n for n in matmul_nodes if n not in current_exclude] + \\\n",
    "                  [n for n in softmax_nodes if n not in current_exclude]\n",
    "\n",
    "print(f\"åˆå§‹ç›¸ä¼¼åº¦: {current_sim:.4f}\\n\")\n",
    "\n",
    "greedy_results = []\n",
    "\n",
    "for iteration in range(min(10, len(remaining_nodes))):\n",
    "    best_sim = current_sim\n",
    "    best_node = None\n",
    "\n",
    "    print(f\"ç¬¬ {iteration+1} è½®: å°è¯•æ·»åŠ  {len(remaining_nodes)} ä¸ªèŠ‚ç‚¹...\")\n",
    "\n",
    "    for test_node in remaining_nodes[:5]:  # åªæµ‹è¯•å‰5ä¸ªï¼ŒåŠ å¿«é€Ÿåº¦\n",
    "        test_exclude = current_exclude + [test_node]\n",
    "        test_path = temp_dir / f\"greedy_{iteration}_{test_node[:20]}.onnx\"\n",
    "\n",
    "        try:\n",
    "            quantize_dynamic(\n",
    "                str(FP32_PATH.parent / \"clip-text-encoder-quant-pre.onnx\"),\n",
    "                str(test_path),\n",
    "                weight_type=QuantType.QInt8,\n",
    "                nodes_to_exclude=test_exclude\n",
    "            )\n",
    "\n",
    "            sim = get_cos_sim(test_path)\n",
    "            print(f\"   + {test_node[:50]:50s}: {sim:.4f}\", end=\"\")\n",
    "\n",
    "            if sim > best_sim:\n",
    "                print(\" âœ¨ æœ€å¥½!\")\n",
    "                best_sim = sim\n",
    "                best_node = test_node\n",
    "            else:\n",
    "                print()\n",
    "\n",
    "            test_path.unlink()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if best_node is None:\n",
    "        print(f\"   æ— æ³•æ”¹è¿›ï¼Œåœæ­¢æœç´¢\\n\")\n",
    "        break\n",
    "\n",
    "    print(f\"âœ… ç¬¬ {iteration+1} è½®æœ€ä¼˜: +{best_node} â†’ {best_sim:.4f}\\n\")\n",
    "\n",
    "    current_exclude.append(best_node)\n",
    "    current_sim = best_sim\n",
    "    remaining_nodes.remove(best_node)\n",
    "\n",
    "    greedy_results.append((f\"ç¬¬{iteration+1}è½®\", current_sim, best_node))\n",
    "\n",
    "# ==================== æ€»ç»“ ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š è¯Šæ–­æ€»ç»“\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if results_matmul:\n",
    "    best_matmul = max(results_matmul, key=lambda x: x[1])\n",
    "    print(f\"\\nğŸ¥‡ MatMul ç­–ç•¥æœ€ä¼˜: {best_matmul[0]}\")\n",
    "    print(f\"   ç›¸ä¼¼åº¦: {best_matmul[1]:.4f}\")\n",
    "\n",
    "if results_softmax:\n",
    "    best_softmax = max(results_softmax, key=lambda x: x[1])\n",
    "    print(f\"\\nğŸ¥‡ Softmax ç­–ç•¥æœ€ä¼˜: {best_softmax[0]}\")\n",
    "    print(f\"   ç›¸ä¼¼åº¦: {best_softmax[1]:.4f}\")\n",
    "\n",
    "if greedy_results:\n",
    "    best_greedy = max(greedy_results, key=lambda x: x[1])\n",
    "    print(f\"\\nğŸ¥‡ è´ªå¿ƒæœç´¢æœ€ä¼˜: {best_greedy[0]}\")\n",
    "    print(f\"   ç›¸ä¼¼åº¦: {best_greedy[1]:.4f}\")\n",
    "    print(f\"   å…³é”®èŠ‚ç‚¹: {best_greedy[2]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# æ¸…ç†\n",
    "shutil.rmtree(temp_dir)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” åˆ†æ ONNX èŠ‚ç‚¹ç»“æ„...\n",
      "\n",
      "ğŸ“Š èŠ‚ç‚¹ç±»å‹ç»Ÿè®¡:\n",
      "   Reshape        : 134 ä¸ª\n",
      "   Transpose      :  74 ä¸ª\n",
      "   Add            :  73 ä¸ª\n",
      "   MatMul         :  61 ä¸ª\n",
      "   Mul            :  50 ä¸ª\n",
      "   Gather         :  37 ä¸ª\n",
      "   Slice          :  36 ä¸ª\n",
      "   Concat         :  30 ä¸ª\n",
      "   LayerNormalization:  25 ä¸ª\n",
      "   Unsqueeze      :  14 ä¸ª\n",
      "   Shape          :  13 ä¸ª\n",
      "   Squeeze        :  13 ä¸ª\n",
      "   Softmax        :  12 ä¸ª\n",
      "   Gemm           :  12 ä¸ª\n",
      "   Sigmoid        :  12 ä¸ª\n",
      "   Range          :   1 ä¸ª\n",
      "      â””â”€ node_arange\n",
      "   ArgMax         :   1 ä¸ª\n",
      "      â””â”€ node_argmax\n",
      "   GatherND       :   1 ä¸ª\n",
      "      â””â”€ node_GatherND_1006\n",
      "\n",
      "ğŸ¯ æ‰€æœ‰ MatMul èŠ‚ç‚¹ (61 ä¸ª):\n",
      "   0: node_MatMul_3\n",
      "   1: node_MatMul_74\n",
      "   2: node_scaled_dot_product_attention\n",
      "   3: node_MatMul_88\n",
      "   4: node_MatMul_91\n",
      "   5: node_MatMul_93\n",
      "   6: node_MatMul_158\n",
      "   7: node_scaled_dot_product_attention_1\n",
      "   8: node_MatMul_171\n",
      "   9: node_MatMul_173\n",
      "   10: node_MatMul_175\n",
      "   11: node_MatMul_240\n",
      "   12: node_scaled_dot_product_attention_2\n",
      "   13: node_MatMul_253\n",
      "   14: node_MatMul_255\n",
      "   15: node_MatMul_257\n",
      "   16: node_MatMul_322\n",
      "   17: node_scaled_dot_product_attention_3\n",
      "   18: node_MatMul_335\n",
      "   19: node_MatMul_337\n",
      "   20: node_MatMul_339\n",
      "   21: node_MatMul_404\n",
      "   22: node_scaled_dot_product_attention_4\n",
      "   23: node_MatMul_417\n",
      "   24: node_MatMul_419\n",
      "   25: node_MatMul_421\n",
      "   26: node_MatMul_486\n",
      "   27: node_scaled_dot_product_attention_5\n",
      "   28: node_MatMul_499\n",
      "   29: node_MatMul_501\n",
      "   30: node_MatMul_503\n",
      "   31: node_MatMul_568\n",
      "   32: node_scaled_dot_product_attention_6\n",
      "   33: node_MatMul_581\n",
      "   34: node_MatMul_583\n",
      "   35: node_MatMul_585\n",
      "   36: node_MatMul_650\n",
      "   37: node_scaled_dot_product_attention_7\n",
      "   38: node_MatMul_663\n",
      "   39: node_MatMul_665\n",
      "   40: node_MatMul_667\n",
      "   41: node_MatMul_732\n",
      "   42: node_scaled_dot_product_attention_8\n",
      "   43: node_MatMul_745\n",
      "   44: node_MatMul_747\n",
      "   45: node_MatMul_749\n",
      "   46: node_MatMul_814\n",
      "   47: node_scaled_dot_product_attention_9\n",
      "   48: node_MatMul_827\n",
      "   49: node_MatMul_829\n",
      "   50: node_MatMul_831\n",
      "   51: node_MatMul_896\n",
      "   52: node_scaled_dot_product_attention_10\n",
      "   53: node_MatMul_909\n",
      "   54: node_MatMul_911\n",
      "   55: node_MatMul_913\n",
      "   56: node_MatMul_978\n",
      "   57: node_scaled_dot_product_attention_11\n",
      "   58: node_MatMul_991\n",
      "   59: node_MatMul_993\n",
      "   60: node_matmul\n",
      "\n",
      "\n",
      "ğŸ§ª æŒ‰èŠ‚ç‚¹æ’åºï¼Œé€ä¸ªé‡åŒ–æµ‹è¯•...\n",
      "======================================================================\n",
      "\n",
      "ç­–ç•¥ A: é€ä¸ªæ’é™¤ MatMul èŠ‚ç‚¹\n",
      "\n",
      "ğŸ”´ å…¨é‡åŒ– (ä»…æ’é™¤ GatherND): 0.8513\n",
      "   æ’é™¤å‰  1 ä¸ª MatMul (å« node_MatMul_3): 0.8521\n",
      "   æ’é™¤å‰  2 ä¸ª MatMul (å« node_MatMul_74): 0.8521\n",
      "   æ’é™¤å‰  3 ä¸ª MatMul (å« node_scaled_dot_product_attention): 0.8521\n",
      "   æ’é™¤å‰  4 ä¸ª MatMul (å« node_MatMul_88): 0.8507\n",
      "   æ’é™¤å‰  5 ä¸ª MatMul (å« node_MatMul_91): 0.9969\n",
      "   æ’é™¤å‰  6 ä¸ª MatMul (å« node_MatMul_93): 0.9971\n",
      "   æ’é™¤å‰  7 ä¸ª MatMul (å« node_MatMul_158): 0.9971\n",
      "   æ’é™¤å‰  8 ä¸ª MatMul (å« node_scaled_dot_product_attention_1): 0.9971\n",
      "   æ’é™¤å‰  9 ä¸ª MatMul (å« node_MatMul_171): 0.9974\n",
      "   æ’é™¤å‰ 10 ä¸ª MatMul (å« node_MatMul_173): 0.9972\n",
      "   æ’é™¤å‰ 11 ä¸ª MatMul (å« node_MatMul_175): 0.9976\n",
      "   æ’é™¤å‰ 12 ä¸ª MatMul (å« node_MatMul_240): 0.9976\n",
      "   æ’é™¤å‰ 13 ä¸ª MatMul (å« node_scaled_dot_product_attention_2): 0.9976\n",
      "   æ’é™¤å‰ 14 ä¸ª MatMul (å« node_MatMul_253): 0.9976\n",
      "   æ’é™¤å‰ 15 ä¸ª MatMul (å« node_MatMul_255): 0.9977\n",
      "   æ’é™¤å‰ 16 ä¸ª MatMul (å« node_MatMul_257): 0.9978\n",
      "   æ’é™¤å‰ 17 ä¸ª MatMul (å« node_MatMul_322): 0.9978\n",
      "   æ’é™¤å‰ 18 ä¸ª MatMul (å« node_scaled_dot_product_attention_3): 0.9978\n",
      "   æ’é™¤å‰ 19 ä¸ª MatMul (å« node_MatMul_335): 0.9978\n",
      "   æ’é™¤å‰ 20 ä¸ª MatMul (å« node_MatMul_337): 0.9978\n",
      "   æ’é™¤å‰ 21 ä¸ª MatMul (å« node_MatMul_339): 0.9979\n",
      "   æ’é™¤å‰ 22 ä¸ª MatMul (å« node_MatMul_404): 0.9979\n",
      "   æ’é™¤å‰ 23 ä¸ª MatMul (å« node_scaled_dot_product_attention_4): 0.9979\n",
      "   æ’é™¤å‰ 24 ä¸ª MatMul (å« node_MatMul_417): 0.9978\n",
      "   æ’é™¤å‰ 25 ä¸ª MatMul (å« node_MatMul_419): 0.9979\n",
      "   æ’é™¤å‰ 26 ä¸ª MatMul (å« node_MatMul_421): 0.9980\n",
      "   æ’é™¤å‰ 27 ä¸ª MatMul (å« node_MatMul_486): 0.9980\n",
      "   æ’é™¤å‰ 28 ä¸ª MatMul (å« node_scaled_dot_product_attention_5): 0.9980\n",
      "   æ’é™¤å‰ 29 ä¸ª MatMul (å« node_MatMul_499): 0.9980\n",
      "   æ’é™¤å‰ 30 ä¸ª MatMul (å« node_MatMul_501): 0.9980\n",
      "   æ’é™¤å‰ 31 ä¸ª MatMul (å« node_MatMul_503): 0.9981\n",
      "   æ’é™¤å‰ 32 ä¸ª MatMul (å« node_MatMul_568): 0.9981\n",
      "   æ’é™¤å‰ 33 ä¸ª MatMul (å« node_scaled_dot_product_attention_6): 0.9981\n",
      "   æ’é™¤å‰ 34 ä¸ª MatMul (å« node_MatMul_581): 0.9981\n",
      "   æ’é™¤å‰ 35 ä¸ª MatMul (å« node_MatMul_583): 0.9982\n",
      "   æ’é™¤å‰ 36 ä¸ª MatMul (å« node_MatMul_585): 0.9982\n",
      "   æ’é™¤å‰ 37 ä¸ª MatMul (å« node_MatMul_650): 0.9982\n",
      "   æ’é™¤å‰ 38 ä¸ª MatMul (å« node_scaled_dot_product_attention_7): 0.9982\n",
      "   æ’é™¤å‰ 39 ä¸ª MatMul (å« node_MatMul_663): 0.9982\n",
      "   æ’é™¤å‰ 40 ä¸ª MatMul (å« node_MatMul_665): 0.9983\n",
      "   æ’é™¤å‰ 41 ä¸ª MatMul (å« node_MatMul_667): 0.9984\n",
      "   æ’é™¤å‰ 42 ä¸ª MatMul (å« node_MatMul_732): 0.9984\n",
      "   æ’é™¤å‰ 43 ä¸ª MatMul (å« node_scaled_dot_product_attention_8): 0.9984\n",
      "   æ’é™¤å‰ 44 ä¸ª MatMul (å« node_MatMul_745): 0.9984\n",
      "   æ’é™¤å‰ 45 ä¸ª MatMul (å« node_MatMul_747): 0.9985\n",
      "   æ’é™¤å‰ 46 ä¸ª MatMul (å« node_MatMul_749): 0.9986\n",
      "   æ’é™¤å‰ 47 ä¸ª MatMul (å« node_MatMul_814): 0.9986\n",
      "   æ’é™¤å‰ 48 ä¸ª MatMul (å« node_scaled_dot_product_attention_9): 0.9986\n",
      "   æ’é™¤å‰ 49 ä¸ª MatMul (å« node_MatMul_827): 0.9986\n",
      "   æ’é™¤å‰ 50 ä¸ª MatMul (å« node_MatMul_829): 0.9987\n",
      "   æ’é™¤å‰ 51 ä¸ª MatMul (å« node_MatMul_831): 0.9988\n",
      "   æ’é™¤å‰ 52 ä¸ª MatMul (å« node_MatMul_896): 0.9988\n",
      "   æ’é™¤å‰ 53 ä¸ª MatMul (å« node_scaled_dot_product_attention_10): 0.9988\n",
      "   æ’é™¤å‰ 54 ä¸ª MatMul (å« node_MatMul_909): 0.9989\n",
      "   æ’é™¤å‰ 55 ä¸ª MatMul (å« node_MatMul_911): 0.9990\n",
      "   æ’é™¤å‰ 56 ä¸ª MatMul (å« node_MatMul_913): 0.9992\n",
      "   æ’é™¤å‰ 57 ä¸ª MatMul (å« node_MatMul_978): 0.9992\n",
      "   æ’é™¤å‰ 58 ä¸ª MatMul (å« node_scaled_dot_product_attention_11): 0.9992\n",
      "   æ’é™¤å‰ 59 ä¸ª MatMul (å« node_MatMul_991): 0.9992\n",
      "   æ’é™¤å‰ 60 ä¸ª MatMul (å« node_MatMul_993): 0.9992\n",
      "   æ’é™¤å‰ 61 ä¸ª MatMul (å« node_matmul): 0.9994\n",
      "\n",
      "ç­–ç•¥ B: é€ä¸ªæ’é™¤ Softmax èŠ‚ç‚¹\n",
      "\n",
      "Softmax èŠ‚ç‚¹æ•°: 12\n",
      "\n",
      "ğŸŸ¡ æ’é™¤æ‰€æœ‰ Softmax: 0.8513\n",
      "\n",
      "ç­–ç•¥ C: è´ªå¿ƒæœç´¢ï¼ˆæ‰¾æœ€æ•æ„Ÿçš„èŠ‚ç‚¹ï¼‰\n",
      "\n",
      "åˆå§‹ç›¸ä¼¼åº¦: 0.8513\n",
      "\n",
      "ç¬¬ 1 è½®: å°è¯•æ·»åŠ  72 ä¸ªèŠ‚ç‚¹...\n",
      "   + node_MatMul_3                                     : 0.8523 âœ¨ æœ€å¥½!\n",
      "   + node_MatMul_74                                    : 0.8513\n",
      "   + node_scaled_dot_product_attention                 : 0.8513\n",
      "   + node_MatMul_88                                    : 0.8505\n",
      "   + node_MatMul_91                                    : 0.9959 âœ¨ æœ€å¥½!\n",
      "âœ… ç¬¬ 1 è½®æœ€ä¼˜: +node_MatMul_91 â†’ 0.9959\n",
      "\n",
      "ç¬¬ 2 è½®: å°è¯•æ·»åŠ  71 ä¸ªèŠ‚ç‚¹...\n",
      "   + node_MatMul_3                                     : 0.9964 âœ¨ æœ€å¥½!\n",
      "   + node_MatMul_74                                    : 0.9959\n",
      "   + node_scaled_dot_product_attention                 : 0.9959\n",
      "   + node_MatMul_88                                    : 0.9970 âœ¨ æœ€å¥½!\n",
      "   + node_MatMul_93                                    : 0.9959\n",
      "âœ… ç¬¬ 2 è½®æœ€ä¼˜: +node_MatMul_88 â†’ 0.9970\n",
      "\n",
      "ç¬¬ 3 è½®: å°è¯•æ·»åŠ  70 ä¸ªèŠ‚ç‚¹...\n",
      "   + node_MatMul_3                                     : 0.9971 âœ¨ æœ€å¥½!\n",
      "   + node_MatMul_74                                    : 0.9970\n",
      "   + node_scaled_dot_product_attention                 : 0.9970\n",
      "   + node_MatMul_93                                    : 0.9969\n",
      "   + node_MatMul_158                                   : 0.9970\n",
      "âœ… ç¬¬ 3 è½®æœ€ä¼˜: +node_MatMul_3 â†’ 0.9971\n",
      "\n",
      "ç¬¬ 4 è½®: å°è¯•æ·»åŠ  69 ä¸ªèŠ‚ç‚¹...\n",
      "   + node_MatMul_74                                    : 0.9971\n",
      "   + node_scaled_dot_product_attention                 : 0.9971\n",
      "   + node_MatMul_93                                    : 0.9973 âœ¨ æœ€å¥½!\n",
      "   + node_MatMul_158                                   : 0.9971\n",
      "   + node_scaled_dot_product_attention_1               : 0.9971\n",
      "âœ… ç¬¬ 4 è½®æœ€ä¼˜: +node_MatMul_93 â†’ 0.9973\n",
      "\n",
      "ç¬¬ 5 è½®: å°è¯•æ·»åŠ  68 ä¸ªèŠ‚ç‚¹...\n",
      "   + node_MatMul_74                                    : 0.9973\n",
      "   + node_scaled_dot_product_attention                 : 0.9973\n",
      "   + node_MatMul_158                                   : 0.9973\n",
      "   + node_scaled_dot_product_attention_1               : 0.9973\n",
      "   + node_MatMul_171                                   : 0.9976 âœ¨ æœ€å¥½!\n",
      "âœ… ç¬¬ 5 è½®æœ€ä¼˜: +node_MatMul_171 â†’ 0.9976\n",
      "\n",
      "ç¬¬ 6 è½®: å°è¯•æ·»åŠ  67 ä¸ªèŠ‚ç‚¹...\n",
      "   + node_MatMul_74                                    : 0.9976\n",
      "   + node_scaled_dot_product_attention                 : 0.9976\n",
      "   + node_MatMul_158                                   : 0.9976\n",
      "   + node_scaled_dot_product_attention_1               : 0.9976\n",
      "   + node_MatMul_173                                   : 0.9974\n",
      "   æ— æ³•æ”¹è¿›ï¼Œåœæ­¢æœç´¢\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š è¯Šæ–­æ€»ç»“\n",
      "======================================================================\n",
      "\n",
      "ğŸ¥‡ MatMul ç­–ç•¥æœ€ä¼˜: æ’é™¤å‰ 61 ä¸ª MatMul\n",
      "   ç›¸ä¼¼åº¦: 0.9994\n",
      "\n",
      "ğŸ¥‡ Softmax ç­–ç•¥æœ€ä¼˜: æ’é™¤æ‰€æœ‰ Softmax\n",
      "   ç›¸ä¼¼åº¦: 0.8513\n",
      "\n",
      "ğŸ¥‡ è´ªå¿ƒæœç´¢æœ€ä¼˜: ç¬¬5è½®\n",
      "   ç›¸ä¼¼åº¦: 0.9976\n",
      "   å…³é”®èŠ‚ç‚¹: node_MatMul_171\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
