{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-18T17:05:34.064145080Z",
     "start_time": "2026-02-18T17:05:34.026569457Z"
    }
   },
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ ONNX Runtime GPU ÈÖçÁΩÆËØäÊñ≠Ôºà‰øÆÂ§çÁâàÔºâ\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==================== 1. PyTorch GPU Ê£ÄÊµã ====================\n",
    "print(\"\\n1Ô∏è‚É£ PyTorch GPU Ê£ÄÊµã:\")\n",
    "print(f\"   CUDA ÂèØÁî®: {torch.cuda.is_available()}\")\n",
    "print(f\"   CUDA ÁâàÊú¨: {torch.version.cuda}\")\n",
    "print(f\"   GPU Êï∞Èáè: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\n   GPU {i}:\")\n",
    "        print(f\"      ÂêçÁß∞: {props.name}\")\n",
    "        print(f\"      ÊòæÂ≠ò: {props.total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"      Compute Capability: {props.major}.{props.minor}\")\n",
    "\n",
    "# ==================== 2. ONNX Runtime ÂèØÁî®Êèê‰æõËÄÖ ====================\n",
    "print(\"\\n\\n2Ô∏è‚É£ ONNX Runtime ÂèØÁî®ÊâßË°åÊèê‰æõËÄÖ:\")\n",
    "available_providers = ort.get_available_providers()\n",
    "print(f\"   ÊâÄÊúâÊèê‰æõËÄÖ: {available_providers}\")\n",
    "\n",
    "if 'CUDAExecutionProvider' in available_providers:\n",
    "    print(\"   ‚úÖ CUDAExecutionProvider ÂèØÁî®\")\n",
    "else:\n",
    "    print(\"   ‚ùå CUDAExecutionProvider ‰∏çÂèØÁî®\")\n",
    "\n",
    "# ==================== 3. CPU vs GPU ÊÄßËÉΩÂØπÊØî ====================\n",
    "print(\"\\n\\n3Ô∏è‚É£ CPU vs GPU ÊÄßËÉΩÂØπÊØî:\")\n",
    "\n",
    "input_size = (1, 3, 224, 224)\n",
    "test_input = np.random.randn(*input_size).astype(np.float32)\n",
    "\n",
    "print(f\"\\n   ÊµãËØïËæìÂÖ•ÂΩ¢Áä∂: {input_size}\")\n",
    "\n",
    "MODEL_PATH = Path(\"../../model/clip-image-encoder.onnx\")\n",
    "\n",
    "if not MODEL_PATH.exists():\n",
    "    print(f\"   ‚ö†Ô∏è  Êâæ‰∏çÂà∞Ê®°Âûã: {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ ÊâæÂà∞Ê®°Âûã: {MODEL_PATH}\")\n",
    "\n",
    "    # ========== CPU Êé®ÁêÜ ==========\n",
    "    print(\"\\n   üîµ CPU Êé®ÁêÜÊµãËØï:\")\n",
    "    opts = ort.SessionOptions()\n",
    "    opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "    sess_cpu = ort.InferenceSession(str(MODEL_PATH), opts,\n",
    "                                    providers=['CPUExecutionProvider'])\n",
    "    input_name = sess_cpu.get_inputs()[0].name\n",
    "\n",
    "    # È¢ÑÁÉ≠\n",
    "    for _ in range(3):\n",
    "        sess_cpu.run(None, {input_name: test_input})\n",
    "\n",
    "    # ËÆ°Êó∂\n",
    "    times_cpu = []\n",
    "    for _ in range(10):\n",
    "        start = time.perf_counter()\n",
    "        sess_cpu.run(None, {input_name: test_input})\n",
    "        times_cpu.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "    cpu_avg = np.mean(times_cpu)\n",
    "    cpu_std = np.std(times_cpu)\n",
    "    print(f\"      Âπ≥ÂùáÂª∂Ëøü: {cpu_avg:.2f} ms ¬± {cpu_std:.2f} ms\")\n",
    "\n",
    "    # ========== GPU Êé®ÁêÜ ==========\n",
    "    if 'CUDAExecutionProvider' in available_providers:\n",
    "        print(\"\\n   üü† GPU Êé®ÁêÜÊµãËØï:\")\n",
    "\n",
    "        # ‚úÖ ÁÆÄÂåñÁöÑÈÖçÁΩÆÔºàÂÖºÂÆπÔºâ\n",
    "        providers = [\n",
    "            ('CUDAExecutionProvider', {\n",
    "                'device_id': 0,\n",
    "            }),\n",
    "            'CPUExecutionProvider'\n",
    "        ]\n",
    "\n",
    "        sess_gpu = ort.InferenceSession(str(MODEL_PATH), opts, providers=providers)\n",
    "\n",
    "        print(f\"      ÂÆûÈôÖ‰ΩøÁî®ÁöÑÊèê‰æõËÄÖ: {sess_gpu.get_providers()}\")\n",
    "\n",
    "        # È¢ÑÁÉ≠\n",
    "        print(\"      È¢ÑÁÉ≠‰∏≠...\")\n",
    "        for _ in range(5):\n",
    "            sess_gpu.run(None, {input_name: test_input})\n",
    "\n",
    "        # ËÆ°Êó∂\n",
    "        print(\"      Êé®ÁêÜ‰∏≠...\")\n",
    "        times_gpu = []\n",
    "        for _ in range(10):\n",
    "            start = time.perf_counter()\n",
    "            sess_gpu.run(None, {input_name: test_input})\n",
    "            times_gpu.append((time.perf_counter() - start) * 1000)\n",
    "\n",
    "        gpu_avg = np.mean(times_gpu)\n",
    "        gpu_std = np.std(times_gpu)\n",
    "        print(f\"      Âπ≥ÂùáÂª∂Ëøü: {gpu_avg:.2f} ms ¬± {gpu_std:.2f} ms\")\n",
    "\n",
    "        # Âä†ÈÄüÊØî\n",
    "        speedup = cpu_avg / gpu_avg\n",
    "        print(f\"\\n   üöÄ Âä†ÈÄüÊØî: {speedup:.2f}x\")\n",
    "\n",
    "        if speedup > 1.5:\n",
    "            print(f\"      ‚úÖ GPU Ê≠£Â∏∏Â∑•‰ΩúÔºÅ\")\n",
    "        elif speedup > 0.8:\n",
    "            print(f\"      ‚ö†Ô∏è  GPU Âä†ÈÄüÊïàÊûú‰∏ÄËà¨\")\n",
    "        else:\n",
    "            print(f\"      ‚ùå GPU Êú™Ë¢´Ê≠£Á°Æ‰ΩøÁî®\")\n",
    "    else:\n",
    "        print(\"\\n   üî¥ GPU ‰∏çÂèØÁî®\")\n",
    "\n",
    "# ==================== ÊÄªÁªì ====================\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"üìä ËØäÊñ≠ÊÄªÁªì\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if torch.cuda.is_available() and 'CUDAExecutionProvider' in available_providers:\n",
    "    print(\"\\n‚úÖ GPU ÁéØÂ¢ÉÈÖçÁΩÆÊ≠£Â∏∏ÔºÅ\")\n",
    "else:\n",
    "    print(\"\\n‚ùå GPU ÁéØÂ¢ÉÊúâÈóÆÈ¢ò\")\n",
    "\n",
    "print(\"=\"*70)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üß™ ONNX Runtime GPU ÈÖçÁΩÆËØäÊñ≠Ôºà‰øÆÂ§çÁâàÔºâ\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£ PyTorch GPU Ê£ÄÊµã:\n",
      "   CUDA ÂèØÁî®: True\n",
      "   CUDA ÁâàÊú¨: 12.8\n",
      "   GPU Êï∞Èáè: 1\n",
      "\n",
      "   GPU 0:\n",
      "      ÂêçÁß∞: NVIDIA GeForce RTX 5070\n",
      "      ÊòæÂ≠ò: 12.34 GB\n",
      "      Compute Capability: 12.0\n",
      "\n",
      "\n",
      "2Ô∏è‚É£ ONNX Runtime ÂèØÁî®ÊâßË°åÊèê‰æõËÄÖ:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'onnxruntime' has no attribute 'get_available_providers'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# ==================== 2. ONNX Runtime ÂèØÁî®Êèê‰æõËÄÖ ====================\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m2Ô∏è‚É£ ONNX Runtime ÂèØÁî®ÊâßË°åÊèê‰æõËÄÖ:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 27\u001B[0m available_providers \u001B[38;5;241m=\u001B[39m \u001B[43mort\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_available_providers\u001B[49m()\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m   ÊâÄÊúâÊèê‰æõËÄÖ: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavailable_providers\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCUDAExecutionProvider\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m available_providers:\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'onnxruntime' has no attribute 'get_available_providers'"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
