{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Install Requirements",
   "id": "23b96568b47cdf56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T03:53:36.055663911Z",
     "start_time": "2026-02-18T03:53:30.607163477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. å½»åº•æ¸…ç†æ—§ç¯å¢ƒ\n",
    "# %pip uninstall -y torch torchvision torchaudio onnx onnxruntime onnxruntime-gpu\n",
    "\n",
    "# 2. å®‰è£…é€‚é… RTX 50 (Blackwell) çš„ PyTorch Nightly\n",
    "%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "\n",
    "# 3. å®‰è£…æœ€æ–°ç‰ˆ ONNX å·¥å…·é“¾\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install onnx>=1.16.0 onnxconverter-common>=1.13.0 onnxruntime-gpu>=1.18.0\n",
    "%pip install onnxscript\n",
    "%pip install onnxruntime\n",
    "%pip install tqdm\n",
    "%pip install scipy\n",
    "%pip install pycocotools\n",
    "\n",
    "import torch\n",
    "print(f\"ğŸ”¥ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"ğŸš€ CUDA Available: {torch.cuda.is_available()}\")"
   ],
   "id": "be1d77882741005f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages/distutils-precedence.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"<frozen site>\", line 195, in addpackage\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\r\n",
      "Requirement already satisfied: torch in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (2.12.0.dev20260217+cu128)\r\n",
      "Requirement already satisfied: torchvision in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (0.26.0.dev20260217+cu128)\r\n",
      "Requirement already satisfied: torchaudio in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (2.11.0.dev20260217+cu128)\r\n",
      "Requirement already satisfied: filelock in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (3.20.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (3.6.1)\r\n",
      "Requirement already satisfied: jinja2 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (2026.2.0)\r\n",
      "Requirement already satisfied: cuda-toolkit==12.8.1 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (12.8.1)\r\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (12.9.4)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.17.1.4 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (9.17.1.4)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.28.9 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (2.28.9)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (3.4.5)\r\n",
      "Requirement already satisfied: triton==3.6.0+git9844da95 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch) (3.6.0+git9844da95)\r\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-bindings==12.9.4->torch) (1.2.2)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (12.8.4.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (11.3.3.83)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (1.13.1.3)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (10.3.9.90)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (11.7.3.90)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (12.5.8.93)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch) (12.8.90)\r\n",
      "Requirement already satisfied: numpy in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torchvision) (2.4.2)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torchvision) (12.1.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Error processing line 1 of /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages/distutils-precedence.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"<frozen site>\", line 195, in addpackage\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Collecting git+https://github.com/openai/CLIP.git\r\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-9_t8qyae\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-9_t8qyae\r\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: ftfy in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from clip==1.0) (6.3.1)\r\n",
      "Requirement already satisfied: packaging in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from clip==1.0) (26.0)\r\n",
      "Requirement already satisfied: regex in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from clip==1.0) (2026.1.15)\r\n",
      "Requirement already satisfied: tqdm in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from clip==1.0) (4.67.3)\r\n",
      "Requirement already satisfied: torch in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from clip==1.0) (2.12.0.dev20260217+cu128)\r\n",
      "Requirement already satisfied: torchvision in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from clip==1.0) (0.26.0.dev20260217+cu128)\r\n",
      "Requirement already satisfied: wcwidth in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.6.0)\r\n",
      "Requirement already satisfied: filelock in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (3.20.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (3.6.1)\r\n",
      "Requirement already satisfied: jinja2 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (2026.2.0)\r\n",
      "Requirement already satisfied: cuda-toolkit==12.8.1 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (12.8.1)\r\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (12.9.4)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.17.1.4 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (9.17.1.4)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.28.9 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (2.28.9)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (3.4.5)\r\n",
      "Requirement already satisfied: triton==3.6.0+git9844da95 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torch->clip==1.0) (3.6.0+git9844da95)\r\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-bindings==12.9.4->torch->clip==1.0) (1.2.2)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (12.8.4.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (11.3.3.83)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (1.13.1.3)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (10.3.9.90)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (11.7.3.90)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (12.5.8.93)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90.* in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from cuda-toolkit[cublas,cudart,cufft,cufile,cupti,curand,cusolver,cusparse,nvjitlink,nvrtc,nvtx]==12.8.1; platform_system == \"Linux\"->torch->clip==1.0) (12.8.90)\r\n",
      "Requirement already satisfied: numpy in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torchvision->clip==1.0) (2.4.2)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from torchvision->clip==1.0) (12.1.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (3.0.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "zsh:1: 1.16.0 not found\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Error processing line 1 of /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages/distutils-precedence.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"<frozen site>\", line 195, in addpackage\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Requirement already satisfied: onnxscript in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (0.6.2)\r\n",
      "Requirement already satisfied: ml_dtypes in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnxscript) (0.5.4)\r\n",
      "Requirement already satisfied: numpy in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnxscript) (2.4.2)\r\n",
      "Requirement already satisfied: onnx_ir<2,>=0.1.15 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnxscript) (0.1.16)\r\n",
      "Requirement already satisfied: onnx>=1.17 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnxscript) (1.20.1)\r\n",
      "Requirement already satisfied: packaging in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnxscript) (26.0)\r\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnxscript) (4.15.0)\r\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnx>=1.17->onnxscript) (6.33.5)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Error processing line 1 of /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages/distutils-precedence.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"<frozen site>\", line 195, in addpackage\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Requirement already satisfied: onnxruntime in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (1.24.1)\r\n",
      "Requirement already satisfied: flatbuffers in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnxruntime) (25.12.19)\r\n",
      "Requirement already satisfied: numpy>=1.21.6 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnxruntime) (2.4.2)\r\n",
      "Requirement already satisfied: packaging in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnxruntime) (26.0)\r\n",
      "Requirement already satisfied: protobuf in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnxruntime) (6.33.5)\r\n",
      "Requirement already satisfied: sympy in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from onnxruntime) (1.14.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from sympy->onnxruntime) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Error processing line 1 of /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages/distutils-precedence.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"<frozen site>\", line 195, in addpackage\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Requirement already satisfied: tqdm in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (4.67.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Error processing line 1 of /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages/distutils-precedence.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"<frozen site>\", line 195, in addpackage\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Requirement already satisfied: scipy in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (1.17.0)\r\n",
      "Requirement already satisfied: numpy<2.7,>=1.26.4 in /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages (from scipy) (2.4.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ğŸ”¥ PyTorch Version: 2.12.0.dev20260217+cu128\n",
      "ğŸš€ CUDA Available: True\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Export Image Tower to ONNX",
   "id": "d6cb79af872ba1fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T03:15:59.231094126Z",
     "start_time": "2026-02-18T03:15:53.666613921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.onnx\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. å¼ºåˆ¶ä½¿ç”¨ CPU è¿›è¡Œå¯¼å‡º (ä¸ºäº†æœ€å¤§åŒ– Android å…¼å®¹æ€§)\n",
    "device = \"cpu\"\n",
    "\n",
    "print(\"æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹...\")\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "vit = model.visual\n",
    "vit.eval()\n",
    "\n",
    "image_path = Path(\"image.jpeg\")\n",
    "\n",
    "if not image_path.exists():\n",
    "    raise FileNotFoundError(f\"æ‰¾ä¸åˆ°å›¾ç‰‡ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {image_path.absolute()}\")\n",
    "\n",
    "print(f\"æ­£åœ¨è¯»å–å›¾ç‰‡: {image_path}\")\n",
    "i = Image.open(image_path)\n",
    "\n",
    "input_tensor: Tensor = preprocess(i).unsqueeze(0).to(device)\n",
    "output_dir = Path(\"../../model\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_output_path = output_dir / \"clip-image-encoder.onnx\"\n",
    "print(f\"ğŸš€ å¼€å§‹å¯¼å‡º Image Encoder\")\n",
    "dynamic_shapes = {\n",
    "    \"x\": {0: torch.export.Dim(\"batch\", min=1, max=32)}\n",
    "}\n",
    "torch.onnx.export(\n",
    "    vit,\n",
    "    (input_tensor,),\n",
    "    str(model_output_path),\n",
    "    export_params=True,\n",
    "    opset_version=18,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_shapes=dynamic_shapes,\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}},\n",
    "    input_names=['input'],\n",
    "    output_names=['output']\n",
    ")\n",
    "\n",
    "print(f\"âœ… å¯¼å‡ºæˆåŠŸï¼æ–‡ä»¶ä¿å­˜åœ¨å½“å‰ç›®å½•: {model_output_path}\")"
   ],
   "id": "38d01da9e14e208b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹...\n",
      "æ­£åœ¨è¯»å–å›¾ç‰‡: image.jpeg\n",
      "ğŸš€ å¼€å§‹å¯¼å‡º Image Encoder\n",
      "[torch.onnx] Obtain model graph for `VisionTransformer([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `VisionTransformer([...]` with `torch.export.export(..., strict=False)`... âœ…\n",
      "[torch.onnx] Run decompositions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guiyan/miniforge3/envs/clip/lib/python3.11/copyreg.py:105: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decompositions... âœ…\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... âœ…\n",
      "[torch.onnx] Optimize the ONNX graph...\n",
      "Applied 13 of general pattern rewrite rules.\n",
      "[torch.onnx] Optimize the ONNX graph... âœ…\n",
      "âœ… å¯¼å‡ºæˆåŠŸï¼æ–‡ä»¶ä¿å­˜åœ¨å½“å‰ç›®å½•: ../../model/clip-image-encoder.onnx\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Quantization Image Model",
   "id": "16bdf9379fd3e12d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T03:31:23.473392162Z",
     "start_time": "2026-02-18T03:31:17.675944244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType, quant_pre_process\n",
    "\n",
    "model_dir = Path(\"../../model\")\n",
    "model_image = model_dir / \"clip-image-encoder.onnx\"\n",
    "model_image_prep = model_dir / \"clip-image-encoder-quant-pre.onnx\"\n",
    "model_image_quant = model_dir / \"clip-image-encoder-quant-int8.onnx\"\n",
    "\n",
    "\n",
    "if not model_image.exists():\n",
    "    raise FileNotFoundError(f\"æ‰¾ä¸åˆ°åŸå§‹æ¨¡å‹: {model_image.absolute()}\")\n",
    "\n",
    "\n",
    "print(\"ğŸ”¨ æ­£åœ¨è¿›è¡Œé‡åŒ–é¢„å¤„ç†...\")\n",
    "quant_pre_process(\n",
    "    input_model=str(model_image),\n",
    "    output_model_path=str(model_image_prep)\n",
    ")\n",
    "\n",
    "\n",
    "print(\"ğŸ“‰ æ­£åœ¨è¿›è¡Œ INT8 åŠ¨æ€é‡åŒ–...\")\n",
    "\n",
    "quantize_dynamic(\n",
    "    model_input=str(model_image_prep),\n",
    "    model_output=str(model_image_quant),\n",
    "    weight_type=QuantType.QInt8,\n",
    "    nodes_to_exclude=['/conv1/Conv']\n",
    ")\n",
    "\n",
    "print(f\"âœ… é‡åŒ–å®Œæˆï¼\")\n",
    "\n",
    "original_size = (model_image.stat().st_size + Path(str(model_image)+\".data\").stat().st_size) / (1024 * 1024)\n",
    "quantized_size = model_image_quant.stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f\"ğŸ“Š åŸå§‹æ€»ä½“ç§¯ (å«.data): {original_size:.2f} MB\")\n",
    "print(f\"ğŸ“Š é‡åŒ–åä½“ç§¯: {quantized_size:.2f} MB\")\n",
    "print(f\"ğŸ“‰ å‹ç¼©æ¯”: {(1 - quantized_size/original_size)*100:.1f}%\")"
   ],
   "id": "aebaea4c36a1779b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¨ æ­£åœ¨è¿›è¡Œé‡åŒ–é¢„å¤„ç†...\n",
      "ğŸ“‰ æ­£åœ¨è¿›è¡Œ INT8 åŠ¨æ€é‡åŒ–...\n",
      "âœ… é‡åŒ–å®Œæˆï¼\n",
      "ğŸ“Š åŸå§‹æ€»ä½“ç§¯ (å«.data): 336.02 MB\n",
      "ğŸ“Š é‡åŒ–åä½“ç§¯: 84.37 MB\n",
      "ğŸ“‰ å‹ç¼©æ¯”: 74.9%\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Benchmark Image Model",
   "id": "ac9b2194b062f7c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T04:02:04.138382752Z",
     "start_time": "2026-02-18T04:01:25.987307197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from torchvision.datasets import Flowers102\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. åŸºç¡€é…ç½®\n",
    "MODEL_DIR = Path(\"../../model\")\n",
    "FP32_PATH = MODEL_DIR / \"clip-image-encoder.onnx\"\n",
    "INT8_PATH = MODEL_DIR / \"clip-image-encoder-quant-int8.onnx\"\n",
    "DATA_ROOT = Path(\"../../data/dataset\")\n",
    "LIMIT = 1000\n",
    "\n",
    "# 2. é¢„å¤„ç†è®¡æ—¶ (æ¨¡æ‹Ÿ Android ç«¯å•å¼ å¤„ç†é€»è¾‘)\n",
    "print(f\"ğŸ“¦ æ­£åœ¨æµ‹è¯•é¢„å¤„ç†è€—æ—¶ ({LIMIT} å¼ å›¾ç‰‡)...\")\n",
    "dataset = Flowers102(root=str(DATA_ROOT), split=\"test\", download=True)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "preprocessed_tensors = []\n",
    "start_pre = time.perf_counter()\n",
    "\n",
    "for i in tqdm(range(LIMIT), desc=\"Preprocessing\"):\n",
    "    img, _ = dataset[i]\n",
    "    tensor = preprocess(img).unsqueeze(0).numpy()\n",
    "    preprocessed_tensors.append(tensor)\n",
    "\n",
    "end_pre = time.perf_counter()\n",
    "lat_pre = (end_pre - start_pre) / LIMIT * 1000  # å¹³å‡æ¯å¼ é¢„å¤„ç†è€—æ—¶ (ms)\n",
    "\n",
    "# 3. åˆå§‹åŒ–ä¼šè¯\n",
    "opts = ort.SessionOptions()\n",
    "opts.intra_op_num_threads = 8\n",
    "sess_fp32 = ort.InferenceSession(str(FP32_PATH), opts, providers=['CPUExecutionProvider'])\n",
    "sess_int8 = ort.InferenceSession(str(INT8_PATH), opts, providers=['CPUExecutionProvider'])\n",
    "\n",
    "# 4. æ ¸å¿ƒæ¨ç†å¯¹æ¯”æµ‹è¯•å‡½æ•°\n",
    "def run_pure_inference(session, tensors, name):\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    # é¢„çƒ­\n",
    "    for i in range(10):\n",
    "        session.run(None, {input_name: tensors[i]})\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    outputs = []\n",
    "    for tensor in tqdm(tensors, desc=f\"Inference ({name})\"):\n",
    "        out = session.run(None, {input_name: tensor})[0]\n",
    "        outputs.append(out)\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    avg_latency = (end_time - start_time) / len(tensors) * 1000\n",
    "    return avg_latency, np.concatenate(outputs)\n",
    "\n",
    "# è¿è¡Œæ¨ç†æµ‹è¯•\n",
    "lat_fp32, feats_fp32 = run_pure_inference(sess_fp32, preprocessed_tensors, \"FP32\")\n",
    "lat_int8, feats_int8 = run_pure_inference(sess_int8, preprocessed_tensors, \"INT8\")\n",
    "\n",
    "# 5. è®¡ç®—æŒ‡æ ‡\n",
    "speedup_pure = lat_fp32 / lat_int8\n",
    "total_lat_int8 = lat_pre + lat_int8  # æ€» Pipeline è€—æ—¶\n",
    "\n",
    "# ç²¾åº¦ä¸å­˜å‚¨\n",
    "sims = torch.nn.functional.cosine_similarity(torch.from_numpy(feats_fp32), torch.from_numpy(feats_int8))\n",
    "avg_sim = sims.mean().item()\n",
    "size_fp32 = (FP32_PATH.stat().st_size + Path(str(FP32_PATH)+\".data\").stat().st_size) / (1024*1024)\n",
    "size_int8 = INT8_PATH.stat().st_size / (1024*1024)\n",
    "\n",
    "# 6. æ‰“å°æŠ¥å‘Š\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“Š CLIP Image Encoder å…¨æµç¨‹æ€§èƒ½æŠ¥å‘Š\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ã€å•å¼ å›¾ç‰‡è€—æ—¶åˆ†è§£ã€‘\")\n",
    "print(f\" - é¢„å¤„ç† (Resize/Normalize): {lat_pre:.2f} ms\")\n",
    "print(f\" - çº¯æ¨ç† (INT8 Inference): {lat_int8:.2f} ms\")\n",
    "print(f\" - ğŸ æ€» Pipeline (INT8): {total_lat_int8:.2f} ms\")\n",
    "\n",
    "print(f\"\\nã€æ ¸å¿ƒåŠ é€Ÿæ¯” (INT8 vs FP32)ã€‘\")\n",
    "print(f\" - çº¯æ¨ç†åŠ é€Ÿ: {speedup_pure:.2f}x\")\n",
    "print(f\" - å…¨æµç¨‹åŠ é€Ÿ: {(lat_pre + lat_fp32) / total_lat_int8:.2f}x\")\n",
    "\n",
    "print(f\"\\nã€é‡åŒ–ç²¾åº¦ä¸å­˜å‚¨ã€‘\")\n",
    "print(f\" - å¹³å‡å‘é‡ç›¸ä¼¼åº¦: {avg_sim:.6f}\")\n",
    "print(f\" - å­˜å‚¨ç©ºé—´å¯¹æ¯”: {size_fp32:.2f} MB -> {size_int8:.2f} MB\")\n",
    "\n",
    "print(f\"\\nã€5 ä¸‡å¼ å›¾æ‰«ææ€»è€—æ—¶é¢„ä¼° (INT8)ã€‘\")\n",
    "# å…¬å¼ï¼š(é¢„å¤„ç†æ—¶é—´ + æ¨ç†æ—¶é—´) * 50000 / 1000 / 60\n",
    "total_min = (50000 * total_lat_int8) / 1000 / 60\n",
    "print(f\" - é¢„è®¡æ‰«ææ€»ç”¨æ—¶: {total_min:.2f} åˆ†é’Ÿ\")\n",
    "print(f\" - å…¶ä¸­é¢„å¤„ç†å æ€»æ¯”: {(lat_pre / total_lat_int8) * 100:.1f}%\")\n",
    "print(\"=\"*50)"
   ],
   "id": "db39f15743adca62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ æ­£åœ¨æµ‹è¯•é¢„å¤„ç†è€—æ—¶ (1000 å¼ å›¾ç‰‡)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:04<00:00, 228.32it/s]\n",
      "Inference (FP32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:21<00:00, 47.28it/s]\n",
      "Inference (INT8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:11<00:00, 85.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ“Š CLIP Image Encoder å…¨æµç¨‹æ€§èƒ½æŠ¥å‘Š\n",
      "==================================================\n",
      "ã€å•å¼ å›¾ç‰‡è€—æ—¶åˆ†è§£ã€‘\n",
      " - é¢„å¤„ç† (Resize/Normalize): 4.38 ms\n",
      " - çº¯æ¨ç† (INT8 Inference): 11.76 ms\n",
      " - ğŸ æ€» Pipeline (INT8): 16.14 ms\n",
      "\n",
      "ã€æ ¸å¿ƒåŠ é€Ÿæ¯” (INT8 vs FP32)ã€‘\n",
      " - çº¯æ¨ç†åŠ é€Ÿ: 1.80x\n",
      " - å…¨æµç¨‹åŠ é€Ÿ: 1.58x\n",
      "\n",
      "ã€é‡åŒ–ç²¾åº¦ä¸å­˜å‚¨ã€‘\n",
      " - å¹³å‡å‘é‡ç›¸ä¼¼åº¦: 0.969175\n",
      " - å­˜å‚¨ç©ºé—´å¯¹æ¯”: 336.02 MB -> 84.37 MB\n",
      "\n",
      "ã€5 ä¸‡å¼ å›¾æ‰«ææ€»è€—æ—¶é¢„ä¼° (INT8)ã€‘\n",
      " - é¢„è®¡æ‰«ææ€»ç”¨æ—¶: 13.45 åˆ†é’Ÿ\n",
      " - å…¶ä¸­é¢„å¤„ç†å æ€»æ¯”: 27.1%\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "# Text Encoder"
   ],
   "id": "b9b68985234585d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TextEncoder Structure\n",
    "\n",
    "copied from Openai.CLIP"
   ],
   "id": "d3b7085eb7dfc25e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T04:07:50.383454400Z",
     "start_time": "2026-02-18T04:07:50.303740776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import clip\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "          embed_dim: int,\n",
    "          # text\n",
    "          context_length: int,\n",
    "          vocab_size: int,\n",
    "          transformer_width: int,\n",
    "          transformer_heads: int,\n",
    "          transformer_layers: int\n",
    "          ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask()\n",
    "        )\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.temperature = nn.Parameter(torch.tensor(0.07))\n",
    "\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "\n",
    "        print(f\"text_projection shape: {self.text_projection.shape}\")\n",
    "        self.dtype = torch.float32\n",
    "\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "\n",
    "        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)\n",
    "        attn_std = self.transformer.width ** -0.5\n",
    "        fc_std = (2 * self.transformer.width) ** -0.5\n",
    "        for block in self.transformer.resblocks:\n",
    "            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)\n",
    "            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)\n",
    "            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)\n",
    "            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)\n",
    "\n",
    "        if self.text_projection is not None:\n",
    "            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)\n",
    "        else:\n",
    "            nn.init.normal_(self.text_projection, std=self.custom_text_config['text_rep_size'] ** -0.5)\n",
    "\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    def forward(self, text):\n",
    "        # print(f'text: {text}')\n",
    "        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]\n",
    "\n",
    "        x = x + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n"
   ],
   "id": "d0115476c36cf2f6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Export Text Tower to ONNX",
   "id": "53feb56c084df2eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T04:47:56.824896116Z",
     "start_time": "2026-02-18T04:47:50.175639730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import clip\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. å¼ºåˆ¶ä½¿ç”¨ CPU\n",
    "device = \"cpu\"\n",
    "\n",
    "print(\"æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹...\")\n",
    "official_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# 2. å®ä¾‹åŒ–è‡ªå®šä¹‰ TextEncoder\n",
    "text_encoder = TextEncoder(\n",
    "    embed_dim=512,\n",
    "    context_length=77,\n",
    "    vocab_size=49408,\n",
    "    transformer_width=512,\n",
    "    transformer_heads=8,\n",
    "    transformer_layers=12\n",
    ").to(device)\n",
    "\n",
    "# 3. çŒè£…æƒé‡\n",
    "text_encoder.load_state_dict(official_model.state_dict(), strict=False)\n",
    "text_encoder.eval()\n",
    "\n",
    "# 4. è·¯å¾„è®¾ç½®\n",
    "output_dir = Path(\"../../model\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_text_path = output_dir / \"clip-text-encoder.onnx\"\n",
    "\n",
    "# 5. å‡†å¤‡æ¨¡æ‹Ÿè¾“å…¥ï¼šæ¨¡æ‹Ÿè¾“å…¥ 2 æ¡æ–‡æœ¬ï¼Œå¢åŠ å¯¼å‡ºæ—¶çš„é²æ£’æ€§\n",
    "dummy_input = torch.ones(2, 77).long().to(device)\n",
    "\n",
    "print(f\"ğŸš€ å¼€å§‹å¯¼å‡º Text Encoder\")\n",
    "\n",
    "dynamic_shapes = {\n",
    "    \"text\": {0: torch.export.Dim(\"batch\", min=1, max=32)}\n",
    "}\n",
    "\n",
    "torch.onnx.export(\n",
    "    text_encoder,\n",
    "    (dummy_input,),\n",
    "    str(model_text_path),\n",
    "    export_params=True,\n",
    "    opset_version=18,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_shapes=dynamic_shapes,\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},\n",
    "        'output': {0: 'batch_size'}\n",
    "    },\n",
    "    input_names=['input'],\n",
    "    output_names=['output']\n",
    ")\n",
    "\n",
    "print(f\"âœ… å¯¼å‡ºæˆåŠŸï¼æ–‡ä»¶ä¿å­˜åœ¨: {model_text_path}\")"
   ],
   "id": "26b7685fba2805fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹...\n",
      "text_projection shape: torch.Size([512, 512])\n",
      "ğŸš€ å¼€å§‹å¯¼å‡º Text Encoder\n",
      "[torch.onnx] Obtain model graph for `TextEncoder([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `TextEncoder([...]` with `torch.export.export(..., strict=False)`... âœ…\n",
      "[torch.onnx] Run decompositions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guiyan/miniforge3/envs/clip/lib/python3.11/copyreg.py:105: FutureWarning: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "  return cls.__new__(cls, *args)\n",
      "W0217 22:47:54.958000 118430 site-packages/torch/onnx/_internal/exporter/_core.py:1253] Tensor 'transformer.resblocks.1.attn_mask' is not one of the initializers\n",
      "W0217 22:47:54.958000 118430 site-packages/torch/onnx/_internal/exporter/_core.py:1253] Tensor 'transformer.resblocks.2.attn_mask' is not one of the initializers\n",
      "W0217 22:47:54.958000 118430 site-packages/torch/onnx/_internal/exporter/_core.py:1253] Tensor 'transformer.resblocks.3.attn_mask' is not one of the initializers\n",
      "W0217 22:47:54.959000 118430 site-packages/torch/onnx/_internal/exporter/_core.py:1253] Tensor 'transformer.resblocks.4.attn_mask' is not one of the initializers\n",
      "W0217 22:47:54.959000 118430 site-packages/torch/onnx/_internal/exporter/_core.py:1253] Tensor 'transformer.resblocks.5.attn_mask' is not one of the initializers\n",
      "W0217 22:47:54.959000 118430 site-packages/torch/onnx/_internal/exporter/_core.py:1253] Tensor 'transformer.resblocks.6.attn_mask' is not one of the initializers\n",
      "W0217 22:47:54.960000 118430 site-packages/torch/onnx/_internal/exporter/_core.py:1253] Tensor 'transformer.resblocks.7.attn_mask' is not one of the initializers\n",
      "W0217 22:47:54.960000 118430 site-packages/torch/onnx/_internal/exporter/_core.py:1253] Tensor 'transformer.resblocks.8.attn_mask' is not one of the initializers\n",
      "W0217 22:47:54.960000 118430 site-packages/torch/onnx/_internal/exporter/_core.py:1253] Tensor 'transformer.resblocks.9.attn_mask' is not one of the initializers\n",
      "W0217 22:47:54.961000 118430 site-packages/torch/onnx/_internal/exporter/_core.py:1253] Tensor 'transformer.resblocks.10.attn_mask' is not one of the initializers\n",
      "W0217 22:47:54.961000 118430 site-packages/torch/onnx/_internal/exporter/_core.py:1253] Tensor 'transformer.resblocks.11.attn_mask' is not one of the initializers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decompositions... âœ…\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... âœ…\n",
      "[torch.onnx] Optimize the ONNX graph...\n",
      "Applied 62 of general pattern rewrite rules.\n",
      "[torch.onnx] Optimize the ONNX graph... âœ…\n",
      "âœ… å¯¼å‡ºæˆåŠŸï¼æ–‡ä»¶ä¿å­˜åœ¨: ../../model/clip-text-encoder.onnx\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Quantization Text Model",
   "id": "5d0bd09522fe3404"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T04:47:45.106619393Z",
     "start_time": "2026-02-18T04:47:41.983432720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType, quant_pre_process\n",
    "\n",
    "# 1. è·¯å¾„è®¾ç½® (å¯¹é½é¡¹ç›®ç»“æ„)\n",
    "model_dir = Path(\"../../model\")\n",
    "model_text = model_dir / \"clip-text-encoder.onnx\"\n",
    "model_text_prep = model_dir / \"clip-text-encoder-quant-pre.onnx\"\n",
    "model_text_quant = model_dir / \"clip-text-encoder-quant-int8.onnx\"\n",
    "\n",
    "# æ£€æŸ¥åŸå§‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "if not model_text.exists():\n",
    "    raise FileNotFoundError(f\"æ‰¾ä¸åˆ°åŸå§‹æ–‡æœ¬æ¨¡å‹: {model_text.absolute()}\")\n",
    "\n",
    "# 2. é‡åŒ–é¢„å¤„ç† (Pre-process)\n",
    "# è¿™ä¸€æ­¥ä¼šä¼˜åŒ–è®¡ç®—å›¾ï¼Œç¡®ä¿ Transformer çš„æ³¨æ„åŠ›æœºåˆ¶åœ¨é‡åŒ–åä¾ç„¶å‡†ç¡®\n",
    "print(\"ğŸ”¨ æ­£åœ¨è¿›è¡Œæ–‡æœ¬æ¨¡å‹é‡åŒ–é¢„å¤„ç†...\")\n",
    "quant_pre_process(\n",
    "    input_model=str(model_text),\n",
    "    output_model_path=str(model_text_prep),\n",
    "    skip_symbolic_shape=True\n",
    ")\n",
    "\n",
    "# 3. æ‰§è¡ŒåŠ¨æ€é‡åŒ– (INT8)\n",
    "print(\"ğŸ“‰ æ­£åœ¨ç”Ÿæˆ INT8 æ–‡æœ¬æ¨¡å‹...\")\n",
    "quantize_dynamic(\n",
    "    model_input=str(model_text_prep),\n",
    "    model_output=str(model_text_quant),\n",
    "    weight_type=QuantType.QInt8,\n",
    ")\n",
    "\n",
    "fp32_onnx = model_dir / \"clip-text-encoder.onnx\"\n",
    "fp32_data = model_dir / \"clip-text-encoder.onnx.data\"\n",
    "\n",
    "size_fp32_total = fp32_onnx.stat().st_size\n",
    "if fp32_data.exists():\n",
    "    size_fp32_total += fp32_data.stat().st_size\n",
    "\n",
    "size_orig = size_fp32_total / (1024 * 1024)\n",
    "size_quant = model_text_quant.stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\"âœ… æ–‡æœ¬æ¨¡å‹é‡åŒ–å®Œæˆï¼\")\n",
    "print(f\"ğŸ“Š åŸå§‹ä½“ç§¯: {size_orig:.2f} MB\")\n",
    "print(f\"ğŸ“Š é‡åŒ–ä½“ç§¯: {size_quant:.2f} MB\")\n",
    "print(f\"ğŸ“‰ å‹ç¼©æ¯”: {(1 - size_quant/size_orig)*100:.1f}%\")\n",
    "print(\"=\"*40)"
   ],
   "id": "2c02687240bbd47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¨ æ­£åœ¨è¿›è¡Œæ–‡æœ¬æ¨¡å‹é‡åŒ–é¢„å¤„ç†...\n",
      "ğŸ“‰ æ­£åœ¨ç”Ÿæˆ INT8 æ–‡æœ¬æ¨¡å‹...\n",
      "\n",
      "========================================\n",
      "âœ… æ–‡æœ¬æ¨¡å‹é‡åŒ–å®Œæˆï¼\n",
      "ğŸ“Š åŸå§‹ä½“ç§¯: 243.27 MB\n",
      "ğŸ“Š é‡åŒ–ä½“ç§¯: 62.07 MB\n",
      "ğŸ“‰ å‹ç¼©æ¯”: 74.5%\n",
      "========================================\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Benchmark Text Model",
   "id": "f8da7feff620a299"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T04:42:29.494271418Z",
     "start_time": "2026-02-18T04:42:07.777453340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. åŸºç¡€é…ç½®\n",
    "MODEL_DIR = Path(\"../../model\")\n",
    "DATA_ROOT = Path(\"../../data/dataset/coco\")\n",
    "FP32_PATH = MODEL_DIR / \"clip-text-encoder.onnx\"\n",
    "INT8_PATH = MODEL_DIR / \"clip-text-encoder-quant-int8.onnx\"\n",
    "\n",
    "# COCO 2017 æ ‡æ³¨æ–‡ä»¶ä¸‹è½½åœ°å€\n",
    "COCO_ANN_URL = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "COCO_ANN_JSON = DATA_ROOT / \"annotations/captions_val2017.json\"\n",
    "LIMIT = 1000\n",
    "\n",
    "# 2. è‡ªåŠ¨åŒ–æ•°æ®ç®¡ç†é€»è¾‘\n",
    "def ensure_coco_annotations():\n",
    "    \"\"\"æ£€æŸ¥å¹¶è‡ªåŠ¨ä¸‹è½½ COCO æ ‡æ³¨æ–‡ä»¶\"\"\"\n",
    "    if COCO_ANN_JSON.exists():\n",
    "        print(f\"âœ… æ‰¾åˆ° COCO æ ‡æ³¨æ–‡ä»¶: {COCO_ANN_JSON}\")\n",
    "        return\n",
    "\n",
    "    print(f\"ğŸ“¦ æ‰¾ä¸åˆ°æ ‡æ³¨æ–‡ä»¶ï¼Œå‡†å¤‡ä»å®˜ç½‘æ‹‰å– (çº¦ 241MB)...\")\n",
    "    DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "    zip_path = DATA_ROOT / \"annotations_trainval2017.zip\"\n",
    "\n",
    "    # ä¸‹è½½è¿›åº¦æ¡\n",
    "    def report_progress(block_num, block_size, total_size):\n",
    "        read_so_far = block_num * block_size\n",
    "        if total_size > 0:\n",
    "            percent = read_so_far * 1e2 / total_size\n",
    "            s = f\"\\ræ­£åœ¨ä¸‹è½½: {percent:5.1f}% [{read_so_far / 1e6:.1f}MB / {total_size / 1e6:.1f}MB]\"\n",
    "            sys.stdout.write(s)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(COCO_ANN_URL, zip_path, reporthook=report_progress)\n",
    "        print(\"\\n\\nè§£å‹ä¸­...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DATA_ROOT)\n",
    "        zip_path.unlink()  # åˆ é™¤å‹ç¼©åŒ…èŠ‚çœç©ºé—´\n",
    "        print(\"âœ¨ æ•°æ®å‡†å¤‡å°±ç»ªï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ä¸‹è½½å¤±è´¥: {e}\")\n",
    "        print(\"è¯·æ£€æŸ¥ç½‘ç»œé“¾æ¥æˆ–å°è¯•æ‰‹åŠ¨ä¸‹è½½æ”¾ç½®ã€‚\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# æ‰§è¡Œæ•°æ®æ£€æŸ¥\n",
    "ensure_coco_annotations()\n",
    "\n",
    "# 3. åŠ è½½çœŸå®è¯­æ–™ (MS COCO)\n",
    "print(f\"ğŸ“¦ æ­£åœ¨ä» MS COCO åŠ è½½çœŸå®äººç±»æ ‡æ³¨...\")\n",
    "try:\n",
    "    with open(COCO_ANN_JSON, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # æå–å‰ LIMIT æ¡æ–‡æœ¬æè¿°\n",
    "    search_queries = [ann['caption'] for ann in data['annotations'][:LIMIT]]\n",
    "    print(f\"âœ… æˆåŠŸåŠ è½½ {len(search_queries)} æ¡ COCO çœŸå®æè¿°ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ è¯»å– JSON å¤±è´¥: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# 4. é¢„å¤„ç†è®¡æ—¶ (BPE Tokenization)\n",
    "print(f\"â±ï¸ æ­£åœ¨æµ‹è¯• Tokenization è€—æ—¶...\")\n",
    "start_pre = time.perf_counter()\n",
    "\n",
    "tokenized_list = []\n",
    "for q in search_queries:\n",
    "    # æ¨¡æ‹Ÿ Android ç«¯çš„é€»è¾‘ï¼šè£å‰ªæˆ–å¡«å……è‡³ 77 ä½\n",
    "    tokens = clip.tokenize([q]).numpy().astype(np.int64)\n",
    "    tokenized_list.append(tokens)\n",
    "\n",
    "end_pre = time.perf_counter()\n",
    "lat_pre = (end_pre - start_pre) / LIMIT * 1000  # å¹³å‡å•æ¡è€—æ—¶ (ms)\n",
    "\n",
    "# 5. åˆå§‹åŒ–æ¨ç†ä¼šè¯\n",
    "opts = ort.SessionOptions()\n",
    "opts.intra_op_num_threads = 8\n",
    "sess_fp32 = ort.InferenceSession(str(FP32_PATH), opts, providers=['CPUExecutionProvider'])\n",
    "sess_int8 = ort.InferenceSession(str(INT8_PATH), opts, providers=['CPUExecutionProvider'])\n",
    "\n",
    "# 6. æ ¸å¿ƒæ¨ç†å¯¹æ¯”\n",
    "def run_text_benchmark(session, tokens_list, name):\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    # é¢„çƒ­\n",
    "    for i in range(10):\n",
    "        session.run(None, {input_name: tokens_list[i]})\n",
    "\n",
    "    latencies = []\n",
    "    features = []\n",
    "\n",
    "    for tokens in tqdm(tokens_list, desc=f\"Inference ({name})\"):\n",
    "        t0 = time.perf_counter()\n",
    "        out = session.run(None, {input_name: tokens})[0]\n",
    "        latencies.append(time.perf_counter() - t0)\n",
    "        features.append(out)\n",
    "\n",
    "    avg_latency = np.mean(latencies) * 1000\n",
    "    return avg_latency, np.concatenate(features)\n",
    "\n",
    "lat_fp32, feats_fp32 = run_text_benchmark(sess_fp32, tokenized_list, \"FP32\")\n",
    "lat_int8, feats_int8 = run_text_benchmark(sess_int8, tokenized_list, \"INT8\")\n",
    "\n",
    "# 7. è®¡ç®—æŒ‡æ ‡\n",
    "speedup_pure = lat_fp32 / lat_int8\n",
    "cos_sim = torch.nn.functional.cosine_similarity(\n",
    "    torch.from_numpy(feats_fp32),\n",
    "    torch.from_numpy(feats_int8)\n",
    ").mean().item()\n",
    "\n",
    "# å­˜å‚¨ç©ºé—´\n",
    "def get_model_size(p):\n",
    "    size = p.stat().st_size\n",
    "    data_p = Path(str(p) + \".data\")\n",
    "    if data_p.exists():\n",
    "        size += data_p.stat().st_size\n",
    "    return size / (1024 * 1024)\n",
    "\n",
    "size_fp32 = get_model_size(FP32_PATH)\n",
    "size_int8 = get_model_size(INT8_PATH)\n",
    "\n",
    "# 8. æ‰“å°æŠ¥å‘Š\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“Š CLIP Text Encoder å®æˆ˜å‹åŠ›æµ‹è¯•æŠ¥å‘Š (MS COCO è‡ªåŠ¨åŒ–ç‰ˆ)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"ã€æ¨ç†æ€§èƒ½åˆ†æã€‘\")\n",
    "print(f\" - BPE åˆ†è¯è€—æ—¶: {lat_pre:.4f} ms\")\n",
    "print(f\" - FP32 å¹³å‡è€—æ—¶: {lat_fp32:.2f} ms\")\n",
    "print(f\" - INT8 å¹³å‡è€—æ—¶: {lat_int8:.2f} ms\")\n",
    "print(f\" - ğŸš€ æ ¸å¿ƒåŠ é€Ÿæ¯”: {speedup_pure:.2f}x\")\n",
    "\n",
    "print(f\"\\nã€é‡åŒ–è´¨é‡ç›‘æ§ã€‘\")\n",
    "print(f\" - å‘é‡å¹³å‡ç›¸ä¼¼åº¦: {cos_sim:.6f}\")\n",
    "print(f\" - ğŸ¯ è¯­ä¹‰å›é€€ (Precision Loss): {(1 - cos_sim)*100:.4f}%\")\n",
    "\n",
    "print(f\"\\nã€èµ„æºå ç”¨å¯¹æ¯”ã€‘\")\n",
    "print(f\" - ç£ç›˜/å†…å­˜å ç”¨: {size_fp32:.2f} MB -> {size_int8:.2f} MB\")\n",
    "print(f\" - ğŸ“‰ èŠ‚çœç©ºé—´: {size_fp32 - size_int8:.2f} MB\")\n",
    "\n",
    "print(f\"\\nã€Android ç«¯å®æˆ˜ä½“æ„Ÿã€‘\")\n",
    "total_int8 = lat_pre + lat_int8\n",
    "print(f\" - ç”¨æˆ·æœç´¢é¢„æœŸå»¶è¿Ÿ: ~{total_int8:.2f} ms\")\n",
    "print(f\" - ä½“éªŒè¯„çº§: {'âš¡ æé€Ÿ (æ— æ„ŸçŸ¥)' if total_int8 < 50 else 'âœ… æµç•…'}\")\n",
    "print(\"=\"*50)"
   ],
   "id": "f25807d6d0ef47de",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰¾åˆ° COCO æ ‡æ³¨æ–‡ä»¶: ../../data/dataset/coco/annotations/captions_val2017.json\n",
      "ğŸ“¦ æ­£åœ¨ä» MS COCO åŠ è½½çœŸå®äººç±»æ ‡æ³¨...\n",
      "âœ… æˆåŠŸåŠ è½½ 1000 æ¡ COCO çœŸå®æè¿°ã€‚\n",
      "â±ï¸ æ­£åœ¨æµ‹è¯• Tokenization è€—æ—¶...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference (FP32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:13<00:00, 75.72it/s]\n",
      "Inference (INT8): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:07<00:00, 125.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ“Š CLIP Text Encoder å®æˆ˜å‹åŠ›æµ‹è¯•æŠ¥å‘Š (MS COCO è‡ªåŠ¨åŒ–ç‰ˆ)\n",
      "==================================================\n",
      "ã€æ¨ç†æ€§èƒ½åˆ†æã€‘\n",
      " - BPE åˆ†è¯è€—æ—¶: 0.0670 ms\n",
      " - FP32 å¹³å‡è€—æ—¶: 13.14 ms\n",
      " - INT8 å¹³å‡è€—æ—¶: 7.93 ms\n",
      " - ğŸš€ æ ¸å¿ƒåŠ é€Ÿæ¯”: 1.66x\n",
      "\n",
      "ã€é‡åŒ–è´¨é‡ç›‘æ§ã€‘\n",
      " - å‘é‡å¹³å‡ç›¸ä¼¼åº¦: 0.830977\n",
      " - ğŸ¯ è¯­ä¹‰å›é€€ (Precision Loss): 16.9023%\n",
      "\n",
      "ã€èµ„æºå ç”¨å¯¹æ¯”ã€‘\n",
      " - ç£ç›˜/å†…å­˜å ç”¨: 243.27 MB -> 62.07 MB\n",
      " - ğŸ“‰ èŠ‚çœç©ºé—´: 181.20 MB\n",
      "\n",
      "ã€Android ç«¯å®æˆ˜ä½“æ„Ÿã€‘\n",
      " - ç”¨æˆ·æœç´¢é¢„æœŸå»¶è¿Ÿ: ~8.00 ms\n",
      " - ä½“éªŒè¯„çº§: âš¡ æé€Ÿ (æ— æ„ŸçŸ¥)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Export to ORT format",
   "id": "1a487e8f1092ab5a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T04:55:05.679026427Z",
     "start_time": "2026-02-18T04:55:03.603738829Z"
    }
   },
   "cell_type": "code",
   "source": "!python -m onnxruntime.tools.convert_onnx_models_to_ort ../../model/clip-image-encoder-quant-int8.onnx",
   "id": "ac55e64e43f46037",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages/distutils-precedence.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"<frozen site>\", line 195, in addpackage\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Converting models with optimization style 'Fixed' and level 'all'\r\n",
      "Converting optimized ONNX model /home/guiyan/project/CLIP-model/model/clip-image-encoder-quant-int8.onnx to ORT format model /home/guiyan/project/CLIP-model/model/clip-image-encoder-quant-int8.ort\r\n",
      "Converted 1/1 models successfully.\r\n",
      "Generating config file from ORT format models with optimization style 'Fixed' and level 'all'\r\n",
      "2026-02-17 22:55:04,814 ort_format_model.utils [INFO] - Created config in /home/guiyan/project/CLIP-model/model/clip-image-encoder-quant-int8.required_operators.config\r\n",
      "Converting models with optimization style 'Runtime' and level 'all'\r\n",
      "Converting optimized ONNX model /home/guiyan/project/CLIP-model/model/clip-image-encoder-quant-int8.onnx to ORT format model /home/guiyan/project/CLIP-model/model/clip-image-encoder-quant-int8.with_runtime_opt.ort\r\n",
      "Converted 1/1 models successfully.\r\n",
      "Converting models again without runtime optimizations to generate a complete config file. These converted models are temporary and will be deleted.\r\n",
      "Converting optimized ONNX model /home/guiyan/project/CLIP-model/model/clip-image-encoder-quant-int8.onnx to ORT format model /home/guiyan/project/CLIP-model/model/tmp3e4xmjm5.without_runtime_opt/clip-image-encoder-quant-int8.ort\r\n",
      "Converted 1/1 models successfully.\r\n",
      "Generating config file from ORT format models with optimization style 'Runtime' and level 'all'\r\n",
      "2026-02-17 22:55:05,390 ort_format_model.utils [INFO] - Created config in /home/guiyan/project/CLIP-model/model/clip-image-encoder-quant-int8.required_operators.with_runtime_opt.config\r\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T04:55:11.396861047Z",
     "start_time": "2026-02-18T04:55:09.592751826Z"
    }
   },
   "cell_type": "code",
   "source": "!python -m onnxruntime.tools.convert_onnx_models_to_ort ../../model/clip-text-encoder-quant-int8.onnx",
   "id": "b5616d77088713e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of /home/guiyan/miniforge3/envs/clip/lib/python3.11/site-packages/distutils-precedence.pth:\r\n",
      "\r\n",
      "  Traceback (most recent call last):\r\n",
      "    File \"<frozen site>\", line 195, in addpackage\r\n",
      "    File \"<string>\", line 1, in <module>\r\n",
      "  ModuleNotFoundError: No module named '_distutils_hack'\r\n",
      "\r\n",
      "Remainder of file ignored\r\n",
      "Converting models with optimization style 'Fixed' and level 'all'\r\n",
      "Converting optimized ONNX model /home/guiyan/project/CLIP-model/model/clip-text-encoder-quant-int8.onnx to ORT format model /home/guiyan/project/CLIP-model/model/clip-text-encoder-quant-int8.ort\r\n",
      "Converted 1/1 models successfully.\r\n",
      "Generating config file from ORT format models with optimization style 'Fixed' and level 'all'\r\n",
      "2026-02-17 22:55:10,699 ort_format_model.utils [INFO] - Created config in /home/guiyan/project/CLIP-model/model/clip-text-encoder-quant-int8.required_operators.config\r\n",
      "Converting models with optimization style 'Runtime' and level 'all'\r\n",
      "Converting optimized ONNX model /home/guiyan/project/CLIP-model/model/clip-text-encoder-quant-int8.onnx to ORT format model /home/guiyan/project/CLIP-model/model/clip-text-encoder-quant-int8.with_runtime_opt.ort\r\n",
      "Converted 1/1 models successfully.\r\n",
      "Converting models again without runtime optimizations to generate a complete config file. These converted models are temporary and will be deleted.\r\n",
      "Converting optimized ONNX model /home/guiyan/project/CLIP-model/model/clip-text-encoder-quant-int8.onnx to ORT format model /home/guiyan/project/CLIP-model/model/tmpw1g9v26a.without_runtime_opt/clip-text-encoder-quant-int8.ort\r\n",
      "Converted 1/1 models successfully.\r\n",
      "Generating config file from ORT format models with optimization style 'Runtime' and level 'all'\r\n",
      "2026-02-17 22:55:11,107 ort_format_model.utils [INFO] - Created config in /home/guiyan/project/CLIP-model/model/clip-text-encoder-quant-int8.required_operators.with_runtime_opt.config\r\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
